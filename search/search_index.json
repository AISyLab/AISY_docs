{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AISY Framework - Deep Learning for Side-Channel Analysis AISY framework is a python-based framework that allows efficient and scalable applications of deep learning to profiling side-channel analysis (SCA). This project was implemented as a result of several years of research on deep learning and side-channel analysis by AisyLab at TU Delft (The Netherlands). Example script AISY Framework allows very easy execution of deep learning in profiled side-channel attacks. Here is an example of all the code that is needed to run a profiled SCA attack on the third key byte (index 2) of an AES implementation from well-known ASCAD dabatase: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() By running the above script, the framework implements automated dataset loading, database configuration, leakage model (labeling) definitions, profiling and attack phases. By default, the framework computes Guessing Entropy and Success Rate metrics. Next, we list all main advantages of AISY Framework for deep learning-based SCA research. Integrated SQLite Database AISY Framework comes with the option to store all analysis results in a SQLite database. Standard libraries are implemented in the framework and users can easily add custom tables to the project. The creation of custom tables does not require any specific background knowlegde on databases. Web Application AISY Framework is built on top of the Flask python-based web framework. A web application is integrated with a web-based user interface. The web application provides a user-friendly way to visualize analysis, plots, results and tables. Note, however that the interface is only intended to provide an easy way to visualize results and keep them organized on databases. From the web interface, the user cannot run scripts or manipulate analysis settings (this will appear in future versions of the framework). Reproducible results with one-click script generation In the Web Application, an user can generate the full script that was used to produce results stored in the database. This is particularly important when the user wishes to reproduce results after he or she changed the script and don't keep track of the changes. Another advantage of this feature can be seen when an user shares the database with a second user. The latter generate all the scripts from original database. State-of-the-art deep learning-based SCA AISY Framework brings state-of-the-art deep learning-based side-channel attacks. We follow recent publications and keep the framework updated accordingly. The current version is 0.2 and it that already provides state-of-the-art features such as custom loss functions, hyperparameter search, visualization, custom metrics and data augmentation. Installation The AISY framework can be cloned from our github page: git clone https://github.com/AISyLab/AISY_Framework.git cd AISY_framework pip install -r requirements.txt Framework layout custom/ # folder containing customized definitions custom_callbacks/callbacks.py # file with user callbacks custom_metrics/ # each .py file contain a custom metric custom_models/neural_networks.py # file to insert user neural networks (keras models) custom_loss/ # each .py file contain a custom loss custom_datasets/datasets.py # file with dataset details custom_data_augmentation/ # file containing data augmentation method custom_tables/tables.py # file containing custom sqlite tables resources/ # folder to store user resources (it will be created when first analysis is executed) databases/ # .sqlite database files with project information and analysis results figures/ # .png figure generated from user models/ # .h5 models npz/ # .npz files with project information and analysis results scripts/ # folder to store main user scripts webapp/ # flask web application files (html, js, css) app.py # main flask application Main Features SCA Metrics (guessing entropy and success rate) Gradient Visualization Data Augmentation Grid Search Random Search Early Stopping Ensemble Custom Metrics Custom Callbacks Confusion Matrix Easy Neural Network Definitions Data Augmentation GUI - plots, tables Automatically generate scripts Fully reproducible scripts Running Scripts The project structure is provided in a way that all scripts are placed inside scripts folder. To run a python file script (e.g., script_aes.py ), the user might use an IDE (like PyCharm, where we simply right-click in the script file and then run ) or, more likely, from command line. To run scripts from command line, user must follow one of the two options: 1) Place the script inside project root folder. 2) Place the script inside scripts folder and use sys python package to indicate the project root folder, as in the example below: import sys sys.path.append('my_path/AISY_framework') import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() Then, simply put the root path location of your AISY_framework project and run the script: python scripts/script_aes.py . Starting the WebApp The Web Application is built on top of Flask web framework. To start the web homepage and visualize project results, simply run the following command on the terminal: cd AISY_Framework/ flask run The user must set the main paths in app.py file (located in root project folder): databases_root_folder = \"my_path/AISY_framework/resources/databases/\" datasets_root_folder = \"my_dataset_folder/\" resources_root_folder = \"my_path/AISY_framework/resources/\"","title":"Home"},{"location":"#welcome-to-aisy-framework-deep-learning-for-side-channel-analysis","text":"AISY framework is a python-based framework that allows efficient and scalable applications of deep learning to profiling side-channel analysis (SCA). This project was implemented as a result of several years of research on deep learning and side-channel analysis by AisyLab at TU Delft (The Netherlands).","title":"Welcome to AISY Framework - Deep Learning for Side-Channel Analysis"},{"location":"#example-script","text":"AISY Framework allows very easy execution of deep learning in profiled side-channel attacks. Here is an example of all the code that is needed to run a profiled SCA attack on the third key byte (index 2) of an AES implementation from well-known ASCAD dabatase: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() By running the above script, the framework implements automated dataset loading, database configuration, leakage model (labeling) definitions, profiling and attack phases. By default, the framework computes Guessing Entropy and Success Rate metrics. Next, we list all main advantages of AISY Framework for deep learning-based SCA research.","title":"Example script"},{"location":"#integrated-sqlite-database","text":"AISY Framework comes with the option to store all analysis results in a SQLite database. Standard libraries are implemented in the framework and users can easily add custom tables to the project. The creation of custom tables does not require any specific background knowlegde on databases.","title":"Integrated SQLite Database"},{"location":"#web-application","text":"AISY Framework is built on top of the Flask python-based web framework. A web application is integrated with a web-based user interface. The web application provides a user-friendly way to visualize analysis, plots, results and tables. Note, however that the interface is only intended to provide an easy way to visualize results and keep them organized on databases. From the web interface, the user cannot run scripts or manipulate analysis settings (this will appear in future versions of the framework).","title":"Web Application"},{"location":"#reproducible-results-with-one-click-script-generation","text":"In the Web Application, an user can generate the full script that was used to produce results stored in the database. This is particularly important when the user wishes to reproduce results after he or she changed the script and don't keep track of the changes. Another advantage of this feature can be seen when an user shares the database with a second user. The latter generate all the scripts from original database.","title":"Reproducible results with one-click script generation"},{"location":"#state-of-the-art-deep-learning-based-sca","text":"AISY Framework brings state-of-the-art deep learning-based side-channel attacks. We follow recent publications and keep the framework updated accordingly. The current version is 0.2 and it that already provides state-of-the-art features such as custom loss functions, hyperparameter search, visualization, custom metrics and data augmentation.","title":"State-of-the-art deep learning-based SCA"},{"location":"#installation","text":"The AISY framework can be cloned from our github page: git clone https://github.com/AISyLab/AISY_Framework.git cd AISY_framework pip install -r requirements.txt","title":"Installation"},{"location":"#framework-layout","text":"custom/ # folder containing customized definitions custom_callbacks/callbacks.py # file with user callbacks custom_metrics/ # each .py file contain a custom metric custom_models/neural_networks.py # file to insert user neural networks (keras models) custom_loss/ # each .py file contain a custom loss custom_datasets/datasets.py # file with dataset details custom_data_augmentation/ # file containing data augmentation method custom_tables/tables.py # file containing custom sqlite tables resources/ # folder to store user resources (it will be created when first analysis is executed) databases/ # .sqlite database files with project information and analysis results figures/ # .png figure generated from user models/ # .h5 models npz/ # .npz files with project information and analysis results scripts/ # folder to store main user scripts webapp/ # flask web application files (html, js, css) app.py # main flask application","title":"Framework layout"},{"location":"#main-features","text":"SCA Metrics (guessing entropy and success rate) Gradient Visualization Data Augmentation Grid Search Random Search Early Stopping Ensemble Custom Metrics Custom Callbacks Confusion Matrix Easy Neural Network Definitions Data Augmentation GUI - plots, tables Automatically generate scripts Fully reproducible scripts","title":"Main Features"},{"location":"#running-scripts","text":"The project structure is provided in a way that all scripts are placed inside scripts folder. To run a python file script (e.g., script_aes.py ), the user might use an IDE (like PyCharm, where we simply right-click in the script file and then run ) or, more likely, from command line. To run scripts from command line, user must follow one of the two options: 1) Place the script inside project root folder. 2) Place the script inside scripts folder and use sys python package to indicate the project root folder, as in the example below: import sys sys.path.append('my_path/AISY_framework') import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() Then, simply put the root path location of your AISY_framework project and run the script: python scripts/script_aes.py .","title":"Running Scripts"},{"location":"#starting-the-webapp","text":"The Web Application is built on top of Flask web framework. To start the web homepage and visualize project results, simply run the following command on the terminal: cd AISY_Framework/ flask run The user must set the main paths in app.py file (located in root project folder): databases_root_folder = \"my_path/AISY_framework/resources/databases/\" datasets_root_folder = \"my_dataset_folder/\" resources_root_folder = \"my_path/AISY_framework/resources/\"","title":"Starting the WebApp"},{"location":"ciphers/","text":"Ciphers Version 0.1 of AISY Framework has support for AES cipher. Future versions will support different ciphers with flexible definitions of leakage models.","title":"Ciphers"},{"location":"ciphers/#ciphers","text":"Version 0.1 of AISY Framework has support for AES cipher. Future versions will support different ciphers with flexible definitions of leakage models.","title":"Ciphers"},{"location":"concepts/","text":"Concepts AISY Framework v0.2 works in a specific setting of known-key side-channel analysis . Therefore, for each analysis, the user must clearly provide with dataset definitions what is the correct key of the target device. Note, however, that automatic labelling is only supported for AES128 cipher. For other ciphers, the labelling process must be provided by the user as custom dataset. AISY Framework Flow Figure below illustrates the main framework flow. In blue, we depict the operations that are basic to the framework (those that are always execute). In light orange, we depict the optional features. The user settings (which are provided in the main script) are shown in yellow. The \"conditional point\" indicates the moment when the framework check a conditional statement in order to verify if a functionality is executed or not. Main Functionalities for Profiling Model Creation Figure belows illustrates the main functionalities implemented in AISY framework regarding profiling model creation. Basically, a user can deploy a single process or hyperparameter search process. For single process, user can train a single model or multiple models on the same dataset and leakage model. This means that in the main script, the user can enter multiple neural networks to be trained by the framework in a single run. In case of hyperparameter search process, user can select between grid or random search. Two additional functionalities are provided with the framework: ProfilingAnalyzer and Ensemble . Profiling Analyzer can be executed in all situations (single model, multi-model or hyperparameter search). Ensemble requires multiple models to be provided by the user, and the ensemble is built from user-defined amount of models.","title":"Concepts"},{"location":"concepts/#concepts","text":"AISY Framework v0.2 works in a specific setting of known-key side-channel analysis . Therefore, for each analysis, the user must clearly provide with dataset definitions what is the correct key of the target device. Note, however, that automatic labelling is only supported for AES128 cipher. For other ciphers, the labelling process must be provided by the user as custom dataset.","title":"Concepts"},{"location":"concepts/#aisy-framework-flow","text":"Figure below illustrates the main framework flow. In blue, we depict the operations that are basic to the framework (those that are always execute). In light orange, we depict the optional features. The user settings (which are provided in the main script) are shown in yellow. The \"conditional point\" indicates the moment when the framework check a conditional statement in order to verify if a functionality is executed or not.","title":"AISY Framework Flow"},{"location":"concepts/#main-functionalities-for-profiling-model-creation","text":"Figure belows illustrates the main functionalities implemented in AISY framework regarding profiling model creation. Basically, a user can deploy a single process or hyperparameter search process. For single process, user can train a single model or multiple models on the same dataset and leakage model. This means that in the main script, the user can enter multiple neural networks to be trained by the framework in a single run. In case of hyperparameter search process, user can select between grid or random search. Two additional functionalities are provided with the framework: ProfilingAnalyzer and Ensemble . Profiling Analyzer can be executed in all situations (single model, multi-model or hyperparameter search). Ensemble requires multiple models to be provided by the user, and the ensemble is built from user-defined amount of models.","title":"Main Functionalities for Profiling Model Creation"},{"location":"custom_callbacks/","text":"Custom Callbacks The user can easily add callbacks to the .fit Keras method. The process is done in two steps as explained below. Step 1: Add Custom Callback Add a custom callback class following the standard structure from the example below: class CustomCallback1(Callback): def __init__(self, x_profiling, y_profiling, plaintexts_profiling, ciphertext_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertext_validation, key_validaton, x_attack, y_attack, plaintexts_attack, ciphertext_attack, key_attack, param, leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, *args): my_args = args[0] # this line is mandatory self.param1 = my_args[0] self.param2 = my_args[1] def on_epoch_end(self, epoch, logs=None): print(\"Processing epoch {}\".format(epoch)) def on_train_end(self, logs=None): pass def get_param1(self): return self.param1 def get_param2(self): return self.param2 The custom callback can be added to the main script or, as recommened for better code organization, to the file custom/custom_callbacks/callbacks.py . In the example above, the CustomCallback1 class expects two parameters, param1 and param2 . In Step 2, we explain how to pass these two parameters in the custom callback. Note that the custom callback must be provided with the following variables in the __init__ function: x_profiling: profiling trace set. y_profiling: categorical profiling set labels. plaintexts_profiling: profiling set plaintext. ciphertexts_profiling: profiling set ciphertexts. key_profiling: profiling set key (can be a different key per trace). x_validation: validation trace set. y_validation: categorical validation set labels. plaintexts_validation: validation set plaintexts. ciphertexts_validation: validation set ciphertexts. key_validation: validation set key. x_attack: attack trace set. y_attack: categorical attack set labels. plaintexts_attack: attack set plaintexts. ciphertexts_attack: attack set ciphertexts. key_attack: attack set key. param: target parameteres. It is a dictionary as in the example below: param = { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 2000 } leakage_model: leakage model parameters.: key_rank_executions: number of key rank executions in the guessing entropy calculatioin. key_rank_report_interval: report trace interval in metric calculations (e.g., guessing entropy, success rate, etc.) key_rank_attack_traces: number of randomly selected traces from attack trace set to be used in each key rank calculation. *args: list of additional and optional parameters to be passed with the custom callback and to be treated inside the custom callback. Step 2: Using Custom Callback The example below show how to use the custom callback in a script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) param1 = [1, 2, 3] param2 = \"my_string\" custom_callbacks = [ { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback1\", \"name\": \"CustomCallback1\", \"parameters\": [param1, param2] }, { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback2\", \"name\": \"CustomCallback2\", \"parameters\": [] } ] aisy.run( callbacks=custom_callbacks ) custom_callbacks = aisy.get_custom_callbacks() custom_callback1 = custom_callbacks[\"CustomCallback1\"] print(custom_callback1.get_param1()) print(custom_callback1.get_param2()) custom_callback2 = custom_callbacks[\"CustomCallback2\"] Note how the structure for custom callback is declared (variable custom_callbacks ). For each custom callback, a dictionary with the following parameters must be defined: class: path to custom callback class. = name: name for the custom callback. parameters: list of parameters to be passed to the custom callback. If no parameters are passed, the item parameters in the dictionary of an element of custom_callbacks must be assigned with [] (empty list), as in the example below: custom_callbacks = [ { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback2\", \"name\": \"CustomCallback2\", \"parameters\": [] } ]","title":"Custom Callbacks"},{"location":"custom_callbacks/#custom-callbacks","text":"The user can easily add callbacks to the .fit Keras method. The process is done in two steps as explained below.","title":"Custom Callbacks"},{"location":"custom_callbacks/#step-1-add-custom-callback","text":"Add a custom callback class following the standard structure from the example below: class CustomCallback1(Callback): def __init__(self, x_profiling, y_profiling, plaintexts_profiling, ciphertext_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertext_validation, key_validaton, x_attack, y_attack, plaintexts_attack, ciphertext_attack, key_attack, param, leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, *args): my_args = args[0] # this line is mandatory self.param1 = my_args[0] self.param2 = my_args[1] def on_epoch_end(self, epoch, logs=None): print(\"Processing epoch {}\".format(epoch)) def on_train_end(self, logs=None): pass def get_param1(self): return self.param1 def get_param2(self): return self.param2 The custom callback can be added to the main script or, as recommened for better code organization, to the file custom/custom_callbacks/callbacks.py . In the example above, the CustomCallback1 class expects two parameters, param1 and param2 . In Step 2, we explain how to pass these two parameters in the custom callback. Note that the custom callback must be provided with the following variables in the __init__ function: x_profiling: profiling trace set. y_profiling: categorical profiling set labels. plaintexts_profiling: profiling set plaintext. ciphertexts_profiling: profiling set ciphertexts. key_profiling: profiling set key (can be a different key per trace). x_validation: validation trace set. y_validation: categorical validation set labels. plaintexts_validation: validation set plaintexts. ciphertexts_validation: validation set ciphertexts. key_validation: validation set key. x_attack: attack trace set. y_attack: categorical attack set labels. plaintexts_attack: attack set plaintexts. ciphertexts_attack: attack set ciphertexts. key_attack: attack set key. param: target parameteres. It is a dictionary as in the example below: param = { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 2000 } leakage_model: leakage model parameters.: key_rank_executions: number of key rank executions in the guessing entropy calculatioin. key_rank_report_interval: report trace interval in metric calculations (e.g., guessing entropy, success rate, etc.) key_rank_attack_traces: number of randomly selected traces from attack trace set to be used in each key rank calculation. *args: list of additional and optional parameters to be passed with the custom callback and to be treated inside the custom callback.","title":"Step 1: Add Custom Callback"},{"location":"custom_callbacks/#step-2-using-custom-callback","text":"The example below show how to use the custom callback in a script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) param1 = [1, 2, 3] param2 = \"my_string\" custom_callbacks = [ { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback1\", \"name\": \"CustomCallback1\", \"parameters\": [param1, param2] }, { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback2\", \"name\": \"CustomCallback2\", \"parameters\": [] } ] aisy.run( callbacks=custom_callbacks ) custom_callbacks = aisy.get_custom_callbacks() custom_callback1 = custom_callbacks[\"CustomCallback1\"] print(custom_callback1.get_param1()) print(custom_callback1.get_param2()) custom_callback2 = custom_callbacks[\"CustomCallback2\"] Note how the structure for custom callback is declared (variable custom_callbacks ). For each custom callback, a dictionary with the following parameters must be defined: class: path to custom callback class. = name: name for the custom callback. parameters: list of parameters to be passed to the custom callback. If no parameters are passed, the item parameters in the dictionary of an element of custom_callbacks must be assigned with [] (empty list), as in the example below: custom_callbacks = [ { \"class\": \"custom.custom_callbacks.callbacks.CustomCallback2\", \"name\": \"CustomCallback2\", \"parameters\": [] } ]","title":"Step 2: Using Custom Callback"},{"location":"custom_database_table/","text":"Adding Custom Tables to the Database User can also add custom SQLite tables to the analysis. The process is rather simple and requires the following steps: Step 1: Define custom table in custom/custom_tables/tables.py In the custom/custom_tables/tables.py file, enter a custom SQLite table structure following the SQLAlchemy package structure. Below, we provide an example of CustomTable class with the following columns: id (Integer): primary key id for entry. value1 (Integer): column with an integer value. value2 (Integer): column with an integer value. value3 (Integer): column with an integer value. datetime (DateTime): data and time for the entry. analysis_id (Integer, ForeignKey): id from Analysis table. Here is the code for the table: class CustomTable(Base): __tablename__ = 'custom_table' id = Column(Integer, primary_key=True) value1 = Column(Integer) value2 = Column(Integer) value3 = Column(Integer) datetime = Column(DateTime, default=datetime.datetime.utcnow) analysis_id = Column(Integer, ForeignKey(Analysis.id)) analysis = relationship(Analysis) def __repr__(self): return \"<CustomTable(id=%d)>\" % self.id The full code for custom/custom_tables/tables.py will look like this: import datetime from sqlalchemy import * from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import relationship from aisy.sca_tables import Analysis from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker Base = declarative_base() def base(): return Base def start_custom_tables(database_name): engine = create_engine('sqlite:///{}'.format(database_name), echo=False) base().metadata.create_all(engine) def start_custom_tables_session(database_name): engine = create_engine('sqlite:///{}'.format(database_name), echo=False) return sessionmaker(bind=engine)() class CustomTable(Base): __tablename__ = 'custom_table' id = Column(Integer, primary_key=True) value1 = Column(Integer) value2 = Column(Integer) value3 = Column(Integer) datetime = Column(DateTime, default=datetime.datetime.utcnow) analysis_id = Column(Integer, ForeignKey(Analysis.id)) analysis = relationship(Analysis) def __repr__(self): return \"<CustomTable(id=%d)>\" % self.id Step 2: Inserting entries to your custom table First, in your script file (located at scripts/ ), create the Aisy() object and set dataset folder , dataset name and database name and run the analysis as usual (look at the imports): import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_tables.tables import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() Next, start the custom table session: start_custom_tables(databases_root_folder + \"database_ascad.sqlite\") session = start_custom_tables_session(databases_root_folder + \"database_ascad.sqlite\") Then, insert your entries to your CustomTable as soon as you define the values to be inserted: new_insert = CustomTable(value1=10, value2=20, value3=30, analysis_id=aisy.settings[\"analysis_id\"]) session.add(new_insert) session.commit() A full script example will look like this: import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_tables.tables import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() start_custom_tables(databases_root_folder + \"database_ascad.sqlite\") session = start_custom_tables_session(databases_root_folder + \"database_ascad.sqlite\") new_insert = CustomTable(value1=10, value2=20, value3=30, analysis_id=aisy.get_analysis_id()) session.add(new_insert) session.commit()","title":"Custom Database Table"},{"location":"custom_database_table/#adding-custom-tables-to-the-database","text":"User can also add custom SQLite tables to the analysis. The process is rather simple and requires the following steps:","title":"Adding Custom Tables to the Database"},{"location":"custom_database_table/#step-1-define-custom-table-in-customcustom_tablestablespy","text":"In the custom/custom_tables/tables.py file, enter a custom SQLite table structure following the SQLAlchemy package structure. Below, we provide an example of CustomTable class with the following columns: id (Integer): primary key id for entry. value1 (Integer): column with an integer value. value2 (Integer): column with an integer value. value3 (Integer): column with an integer value. datetime (DateTime): data and time for the entry. analysis_id (Integer, ForeignKey): id from Analysis table. Here is the code for the table: class CustomTable(Base): __tablename__ = 'custom_table' id = Column(Integer, primary_key=True) value1 = Column(Integer) value2 = Column(Integer) value3 = Column(Integer) datetime = Column(DateTime, default=datetime.datetime.utcnow) analysis_id = Column(Integer, ForeignKey(Analysis.id)) analysis = relationship(Analysis) def __repr__(self): return \"<CustomTable(id=%d)>\" % self.id The full code for custom/custom_tables/tables.py will look like this: import datetime from sqlalchemy import * from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import relationship from aisy.sca_tables import Analysis from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker Base = declarative_base() def base(): return Base def start_custom_tables(database_name): engine = create_engine('sqlite:///{}'.format(database_name), echo=False) base().metadata.create_all(engine) def start_custom_tables_session(database_name): engine = create_engine('sqlite:///{}'.format(database_name), echo=False) return sessionmaker(bind=engine)() class CustomTable(Base): __tablename__ = 'custom_table' id = Column(Integer, primary_key=True) value1 = Column(Integer) value2 = Column(Integer) value3 = Column(Integer) datetime = Column(DateTime, default=datetime.datetime.utcnow) analysis_id = Column(Integer, ForeignKey(Analysis.id)) analysis = relationship(Analysis) def __repr__(self): return \"<CustomTable(id=%d)>\" % self.id","title":"Step 1: Define custom table in custom/custom_tables/tables.py"},{"location":"custom_database_table/#step-2-inserting-entries-to-your-custom-table","text":"First, in your script file (located at scripts/ ), create the Aisy() object and set dataset folder , dataset name and database name and run the analysis as usual (look at the imports): import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_tables.tables import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() Next, start the custom table session: start_custom_tables(databases_root_folder + \"database_ascad.sqlite\") session = start_custom_tables_session(databases_root_folder + \"database_ascad.sqlite\") Then, insert your entries to your CustomTable as soon as you define the values to be inserted: new_insert = CustomTable(value1=10, value2=20, value3=30, analysis_id=aisy.settings[\"analysis_id\"]) session.add(new_insert) session.commit() A full script example will look like this: import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_tables.tables import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() start_custom_tables(databases_root_folder + \"database_ascad.sqlite\") session = start_custom_tables_session(databases_root_folder + \"database_ascad.sqlite\") new_insert = CustomTable(value1=10, value2=20, value3=30, analysis_id=aisy.get_analysis_id()) session.add(new_insert) session.commit()","title":"Step 2: Inserting entries to your custom table"},{"location":"custom_dataset/","text":"Custom datasets Basic h5 dataset format The main Aisy class framework only supports datasets in .h5 format for the current version. The dataset in .h5 format must be structured according to the format created by ANSSI in ASCAD database. The .h5 file must contain traces and metadata fields, as follows: Profiling traces: [Profiling_traces/traces] : 2D array containing profiling traces. The format of the array must be nb_traces x nb_samples . Here, samples mean the amount of data points (or features) in a trace. Attack traces: [Attack_traces/traces] : this field should contain a 2D array with attack traces. The format of the array must be * nb_traces x nb_samples*. Profiling Metadata: [Profiling_traces/metadata] : metadata contains all 2D arrays for keys , plaintext and ciphertext . Note that, depending on the leakage model plaintext or ciphertext are not necessary. If metadata contains mask values, they will automatically ignored by the main Aisy class when creating labels. Attack Metadata: [Attack_traces/metadata] : similarly, this field contains metadata for attack set. Below, user can find an example of how to create a .h5 dataset that can be interpreted by the main Aisy class: import numpy as np import h5py \"\"\" create dummy profiling traces array with 10000 traces, each one containing 100 samples \"\"\" profiling_traces = np.random.rand(10000, 100) \"\"\" create dummy attack traces array with 1000 traces, each one containing 100 samples \"\"\" attack_traces = np.random.rand(1000, 100) \"\"\" create dummy plaintexts, ciphertexts and keys for profiling set: array with 10000 rows x 16 bytes \"\"\" plaintexts_profiling = np.random.randint(0, 256, (10000, 16)) ciphertexts_profiling = np.random.randint(0, 256, (10000, 16)) keys_profiling = np.random.randint(0, 256, (10000, 16)) \"\"\" create dummy plaintexts, ciphertexts and keys for attack set: array with 1000 rows x 16 bytes \"\"\" plaintexts_attack = np.random.randint(0, 256, (1000, 16)) ciphertexts_attack = np.random.randint(0, 256, (1000, 16)) keys_attack = np.random.randint(0, 256, (1000, 16)) nb_profiling_traces = 10000 nb_attack_traces = 1000 profiling_index = [i for i in range(nb_profiling_traces)] attack_index = [i for i in range(nb_attack_traces)] out_file = h5py.File('new_dataset.h5', 'w') profiling_traces_group = out_file.create_group(\"Profiling_traces\") attack_traces_group = out_file.create_group(\"Attack_traces\") \"\"\" create fields for profiling and attack sets \"\"\" profiling_traces_group.create_dataset(name=\"traces\", data=profiling_traces, dtype=profiling_traces.dtype) attack_traces_group.create_dataset(name=\"traces\", data=attack_traces, dtype=attack_traces.dtype) \"\"\" create metadata fields for profiling and attack sets\"\"\" metadata_type_profiling = np.dtype([(\"plaintext\", plaintexts_profiling.dtype, (nb_profiling_traces,)), (\"ciphertext\", ciphertexts_profiling.dtype, (nb_profiling_traces,)), (\"key\", keys_profiling.dtype, (nb_profiling_traces,)) ]) profiling_metadata = np.array([(plaintexts_profiling[n], ciphertexts_profiling[n], keys_profiling[n]) for n in zip(profiling_index)], dtype=metadata_type_profiling) metadata_type_attack = np.dtype([(\"plaintext\", plaintexts_attack.dtype, (nb_attack_traces,)), (\"ciphertext\", ciphertexts_attack.dtype, (nb_attack_traces,)), (\"key\", keys_attack.dtype, (nb_attack_traces,)) ]) profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type_profiling) attack_metadata = np.array([(plaintexts_attack[n], ciphertexts_attack[n], keys_attack[n]) for n in zip(attack_index)], dtype=metadata_type_attack) attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type_attack) out_file.flush() out_file.close() For sanity checks, the user may verify if the .h5 file contain specific field in metadata. The next example shows how to verify if ciphertext field is located in profiling metadata: import h5py h5_file = h5py.File(\"my_dataset.h5\", \"r\") if \"ciphertext\" in h5_file['Profiling_traces/metadata'].dtype.fields: print(\"ciphertext field is in metadata for profiling traces.\") h5py package To avoid incompatibility issues, AISY framework recommends to use h5py version 2.10.0, as specified in requirements.txt file. Creating Dataset object After reading a .h5 file, AISY framework automatically generates an internal Dataset object. This object is manipulated by the classes and functions that uses dataset information. As specified in this documentation, AISY framework automatically generates dataset labels from plaintext (or ciphertexts) and keys from .h5 metadata. However, the current version of the framework only supports labels generation for AES128-related datasets. In case the user wants to label the dataset with an alternative concept (e.g., by taking into consideration masks available to the user), it is possible to create a Dataset object and provided it to the main Aisy class before running the analysis. This way, the analysis is not restricted to AES cipher only. The following code provides an example to generate a Dataset object. Note that labels are created by the user and passed to the my_dataset object. import aisy_sca from app import * from custom.custom_models.neural_networks import * from aisy_sca.datasets.Dataset import Dataset from tensorflow.keras.utils import to_categorical import numpy as np \"\"\" set profiling, validation and attack traces\"\"\" x_profiling = np.random.rand(10000, 100) x_attack = np.random.rand(1000, 100) x_validation = np.random.rand(1000, 100) \"\"\" set profiling, validation and attack labels\"\"\" y_profiling = to_categorical(np.random.randint(0, 256, 10000), num_classes=256) y_attack = to_categorical(np.random.randint(0, 256, 1000), num_classes=256) y_validation = to_categorical(np.random.randint(0, 256, 1000), num_classes=256) \"\"\" create list of key guesses for attack and validation sets \"\"\" labels_key_guess_validation_set = np.random.randint(0, 256, (256, 1000)) labels_key_guess_attack_set = np.random.randint(0, 256, (256, 1000)) \"\"\" create dataset object \"\"\" new_dataset = Dataset(x_profiling, y_profiling, x_attack, y_attack, x_validation, y_validation) new_dataset_dict = { \"filename\": \"new_dataset.h5\", \"key\": \"4DFBE0F27221FE10A78D4ADC8E490469\", \"first_sample\": 0, \"number_of_samples\": 100, \"number_of_profiling_traces\": 10000, \"number_of_attack_traces\": 1000 } aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad_variable.sqlite\") \"\"\" User must set dataset object and value of good key for GE and SR \"\"\" aisy.set_classes(256) aisy.set_good_key(224) aisy.set_dataset(new_dataset_dict, dataset=new_dataset) aisy.set_labels_key_guesses_attack_set(labels_key_guess_attack_set) aisy.set_labels_key_guesses_validation_set(labels_key_guess_validation_set) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.add_neural_network(mlp) aisy.run() In the above example, labels ( y_profiling, y_validation, y_attack ) are generated before the method run() from main Aisy class is called.","title":"Custom Dataset"},{"location":"custom_dataset/#custom-datasets","text":"","title":"Custom datasets"},{"location":"custom_dataset/#basic-h5-dataset-format","text":"The main Aisy class framework only supports datasets in .h5 format for the current version. The dataset in .h5 format must be structured according to the format created by ANSSI in ASCAD database. The .h5 file must contain traces and metadata fields, as follows: Profiling traces: [Profiling_traces/traces] : 2D array containing profiling traces. The format of the array must be nb_traces x nb_samples . Here, samples mean the amount of data points (or features) in a trace. Attack traces: [Attack_traces/traces] : this field should contain a 2D array with attack traces. The format of the array must be * nb_traces x nb_samples*. Profiling Metadata: [Profiling_traces/metadata] : metadata contains all 2D arrays for keys , plaintext and ciphertext . Note that, depending on the leakage model plaintext or ciphertext are not necessary. If metadata contains mask values, they will automatically ignored by the main Aisy class when creating labels. Attack Metadata: [Attack_traces/metadata] : similarly, this field contains metadata for attack set. Below, user can find an example of how to create a .h5 dataset that can be interpreted by the main Aisy class: import numpy as np import h5py \"\"\" create dummy profiling traces array with 10000 traces, each one containing 100 samples \"\"\" profiling_traces = np.random.rand(10000, 100) \"\"\" create dummy attack traces array with 1000 traces, each one containing 100 samples \"\"\" attack_traces = np.random.rand(1000, 100) \"\"\" create dummy plaintexts, ciphertexts and keys for profiling set: array with 10000 rows x 16 bytes \"\"\" plaintexts_profiling = np.random.randint(0, 256, (10000, 16)) ciphertexts_profiling = np.random.randint(0, 256, (10000, 16)) keys_profiling = np.random.randint(0, 256, (10000, 16)) \"\"\" create dummy plaintexts, ciphertexts and keys for attack set: array with 1000 rows x 16 bytes \"\"\" plaintexts_attack = np.random.randint(0, 256, (1000, 16)) ciphertexts_attack = np.random.randint(0, 256, (1000, 16)) keys_attack = np.random.randint(0, 256, (1000, 16)) nb_profiling_traces = 10000 nb_attack_traces = 1000 profiling_index = [i for i in range(nb_profiling_traces)] attack_index = [i for i in range(nb_attack_traces)] out_file = h5py.File('new_dataset.h5', 'w') profiling_traces_group = out_file.create_group(\"Profiling_traces\") attack_traces_group = out_file.create_group(\"Attack_traces\") \"\"\" create fields for profiling and attack sets \"\"\" profiling_traces_group.create_dataset(name=\"traces\", data=profiling_traces, dtype=profiling_traces.dtype) attack_traces_group.create_dataset(name=\"traces\", data=attack_traces, dtype=attack_traces.dtype) \"\"\" create metadata fields for profiling and attack sets\"\"\" metadata_type_profiling = np.dtype([(\"plaintext\", plaintexts_profiling.dtype, (nb_profiling_traces,)), (\"ciphertext\", ciphertexts_profiling.dtype, (nb_profiling_traces,)), (\"key\", keys_profiling.dtype, (nb_profiling_traces,)) ]) profiling_metadata = np.array([(plaintexts_profiling[n], ciphertexts_profiling[n], keys_profiling[n]) for n in zip(profiling_index)], dtype=metadata_type_profiling) metadata_type_attack = np.dtype([(\"plaintext\", plaintexts_attack.dtype, (nb_attack_traces,)), (\"ciphertext\", ciphertexts_attack.dtype, (nb_attack_traces,)), (\"key\", keys_attack.dtype, (nb_attack_traces,)) ]) profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type_profiling) attack_metadata = np.array([(plaintexts_attack[n], ciphertexts_attack[n], keys_attack[n]) for n in zip(attack_index)], dtype=metadata_type_attack) attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type_attack) out_file.flush() out_file.close() For sanity checks, the user may verify if the .h5 file contain specific field in metadata. The next example shows how to verify if ciphertext field is located in profiling metadata: import h5py h5_file = h5py.File(\"my_dataset.h5\", \"r\") if \"ciphertext\" in h5_file['Profiling_traces/metadata'].dtype.fields: print(\"ciphertext field is in metadata for profiling traces.\")","title":"Basic h5 dataset format"},{"location":"custom_dataset/#h5py-package","text":"To avoid incompatibility issues, AISY framework recommends to use h5py version 2.10.0, as specified in requirements.txt file.","title":"h5py package"},{"location":"custom_dataset/#creating-dataset-object","text":"After reading a .h5 file, AISY framework automatically generates an internal Dataset object. This object is manipulated by the classes and functions that uses dataset information. As specified in this documentation, AISY framework automatically generates dataset labels from plaintext (or ciphertexts) and keys from .h5 metadata. However, the current version of the framework only supports labels generation for AES128-related datasets. In case the user wants to label the dataset with an alternative concept (e.g., by taking into consideration masks available to the user), it is possible to create a Dataset object and provided it to the main Aisy class before running the analysis. This way, the analysis is not restricted to AES cipher only. The following code provides an example to generate a Dataset object. Note that labels are created by the user and passed to the my_dataset object. import aisy_sca from app import * from custom.custom_models.neural_networks import * from aisy_sca.datasets.Dataset import Dataset from tensorflow.keras.utils import to_categorical import numpy as np \"\"\" set profiling, validation and attack traces\"\"\" x_profiling = np.random.rand(10000, 100) x_attack = np.random.rand(1000, 100) x_validation = np.random.rand(1000, 100) \"\"\" set profiling, validation and attack labels\"\"\" y_profiling = to_categorical(np.random.randint(0, 256, 10000), num_classes=256) y_attack = to_categorical(np.random.randint(0, 256, 1000), num_classes=256) y_validation = to_categorical(np.random.randint(0, 256, 1000), num_classes=256) \"\"\" create list of key guesses for attack and validation sets \"\"\" labels_key_guess_validation_set = np.random.randint(0, 256, (256, 1000)) labels_key_guess_attack_set = np.random.randint(0, 256, (256, 1000)) \"\"\" create dataset object \"\"\" new_dataset = Dataset(x_profiling, y_profiling, x_attack, y_attack, x_validation, y_validation) new_dataset_dict = { \"filename\": \"new_dataset.h5\", \"key\": \"4DFBE0F27221FE10A78D4ADC8E490469\", \"first_sample\": 0, \"number_of_samples\": 100, \"number_of_profiling_traces\": 10000, \"number_of_attack_traces\": 1000 } aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad_variable.sqlite\") \"\"\" User must set dataset object and value of good key for GE and SR \"\"\" aisy.set_classes(256) aisy.set_good_key(224) aisy.set_dataset(new_dataset_dict, dataset=new_dataset) aisy.set_labels_key_guesses_attack_set(labels_key_guess_attack_set) aisy.set_labels_key_guesses_validation_set(labels_key_guess_validation_set) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.add_neural_network(mlp) aisy.run() In the above example, labels ( y_profiling, y_validation, y_attack ) are generated before the method run() from main Aisy class is called.","title":"Creating Dataset object"},{"location":"custom_loss/","text":"Custom Loss Functions AISY framework allows the user to set a custom loss function for the evaluated models. When a user set a custom loss function in the main Aisy class, this loss function is applied to all models that are trained by the framework. This applies to single model, multi-model or hyperparamerer search.","title":"Custom Loss Functions"},{"location":"custom_loss/#custom-loss-functions","text":"AISY framework allows the user to set a custom loss function for the evaluated models. When a user set a custom loss function in the main Aisy class, this loss function is applied to all models that are trained by the framework. This applies to single model, multi-model or hyperparamerer search.","title":"Custom Loss Functions"},{"location":"custom_metrics/","text":"Custom Early Stopping Metrics AISY Framework allows easy definition of custom metrics to be monitored during training. This is specially important when early stopping is considered in the profiled SCA. Custom metrics are processed in a standard callback, EarlyStoppingCallback . Therefore, an early_stopping dictionary is passed to the run method: early_stopping = { \"metrics\": { \"accuracy\": { \"direction\": \"max\", \"class\": \"accuracy\", \"parameters\": [] }, \"loss\": { \"direction\": \"min\", \"class\": \"loss\", \"parameters\": [] }, \"guessing_entropy\": { \"direction\": \"min\", \"class\": \"guessing_entropy\", \"parameters\": [] }, \"success_rate\": { \"direction\": \"max\", \"class\": \"success_rate\", \"parameters\": [] } } } In the example above, we have four custom metrics. Of course, these four metrics are already implemented as standard metrics in the AISY Framework. However, these standard metrics will only be used in the early stopping mechanism if included in the early_stopping dictionary. Note that for each metric we have three attributes: direction , class and parameters . The direction indicates what is the reference (min or max) to be detected as best achieved value. The class indicates the name of the python file containing the code for the metric and must be placed in custom/custom_metrics/ directory. Step 1: Creating custom metric .py file Create a new .py file named with the attribute class as defined in early_stopping . Following the early_stopping dictionary from the above example, we create four python files inside custom/custom_metrics/ directory: accuracy.py loss.py guessing_entropy.py success_rate.py Standard custom metric file structure (standard parameters) Inside each of this files, create the following basic method run : def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): pass where parameters are explained below x_profiling: profiling trace set. y_profiling: categorical profiling set labels. plaintexts_profiling: profiling set plaintext. ciphertexts_profiling: profiling set ciphertexts. key_profiling: profiling set key (can be a different key per trace). x_validation: validation trace set. y_validation: categorical validation set labels. plaintexts_validation: validation set plaintexts. ciphertexts_validation: validation set ciphertexts. key_validation: validation set key. x_attack: attack trace set. y_attack: categorical attack set labels. plaintexts_attack: attack set plaintexts. ciphertexts_attack: attack set ciphertexts. key_attack: attack set key. param: target parameteres. It is a dictionary as in the example below: param = { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 2000 } aes_leakage_model: leakage model parameters. It is a dictionary as in the example below: aes_leakage_model = { \"leakage_model\": \"HW\", # \"HW\", \"ID\" or \"bit\" \"byte\": byte, } key_rank_executions: number of key rank executions in the guessing entropy calculatioin. key_rank_report_interval: report trace interval in metric calculations (e.g., guessing entropy, success rate, etc.) key_rank_attack_traces: number of randomly selected traces from attack trace set to be used in each key rank calculation. model: keras (neural network) model. *args: list of additional and optional parameters to be passed with early_stopping dictionary. Examples of custom metric files For accuracy.py metric, the code can be defined as in the example below: def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): loss, acc = model.evaluate(x_validation, y_validation, verbose=0) return acc For loss.py metric, the code can be defined as in the example below: def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): loss, _ = model.evaluate(x_validation, y_validation, verbose=0) return loss Below, we provide a full example for guessing_entropy.py where guessing entropy is calculated for the validation trace set having one byte of S-Box output of first AES encryption round as target state. The run method returns the guessing entropy after processing key_rank_attack_traces validation traces. import numpy as np import random from sklearn.utils import shuffle sbox = np.array([ 0x63, 0x7C, 0x77, 0x7B, 0xF2, 0x6B, 0x6F, 0xC5, 0x30, 0x01, 0x67, 0x2B, 0xFE, 0xD7, 0xAB, 0x76, 0xCA, 0x82, 0xC9, 0x7D, 0xFA, 0x59, 0x47, 0xF0, 0xAD, 0xD4, 0xA2, 0xAF, 0x9C, 0xA4, 0x72, 0xC0, 0xB7, 0xFD, 0x93, 0x26, 0x36, 0x3F, 0xF7, 0xCC, 0x34, 0xA5, 0xE5, 0xF1, 0x71, 0xD8, 0x31, 0x15, 0x04, 0xC7, 0x23, 0xC3, 0x18, 0x96, 0x05, 0x9A, 0x07, 0x12, 0x80, 0xE2, 0xEB, 0x27, 0xB2, 0x75, 0x09, 0x83, 0x2C, 0x1A, 0x1B, 0x6E, 0x5A, 0xA0, 0x52, 0x3B, 0xD6, 0xB3, 0x29, 0xE3, 0x2F, 0x84, 0x53, 0xD1, 0x00, 0xED, 0x20, 0xFC, 0xB1, 0x5B, 0x6A, 0xCB, 0xBE, 0x39, 0x4A, 0x4C, 0x58, 0xCF, 0xD0, 0xEF, 0xAA, 0xFB, 0x43, 0x4D, 0x33, 0x85, 0x45, 0xF9, 0x02, 0x7F, 0x50, 0x3C, 0x9F, 0xA8, 0x51, 0xA3, 0x40, 0x8F, 0x92, 0x9D, 0x38, 0xF5, 0xBC, 0xB6, 0xDA, 0x21, 0x10, 0xFF, 0xF3, 0xD2, 0xCD, 0x0C, 0x13, 0xEC, 0x5F, 0x97, 0x44, 0x17, 0xC4, 0xA7, 0x7E, 0x3D, 0x64, 0x5D, 0x19, 0x73, 0x60, 0x81, 0x4F, 0xDC, 0x22, 0x2A, 0x90, 0x88, 0x46, 0xEE, 0xB8, 0x14, 0xDE, 0x5E, 0x0B, 0xDB, 0xE0, 0x32, 0x3A, 0x0A, 0x49, 0x06, 0x24, 0x5C, 0xC2, 0xD3, 0xAC, 0x62, 0x91, 0x95, 0xE4, 0x79, 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65, 0x7A, 0xAE, 0x08, 0xBA, 0x78, 0x25, 0x2E, 0x1C, 0xA6, 0xB4, 0xC6, 0xE8, 0xDD, 0x74, 0x1F, 0x4B, 0xBD, 0x8B, 0x8A, 0x70, 0x3E, 0xB5, 0x66, 0x48, 0x03, 0xF6, 0x0E, 0x61, 0x35, 0x57, 0xB9, 0x86, 0xC1, 0x1D, 0x9E, 0xE1, 0xF8, 0x98, 0x11, 0x69, 0xD9, 0x8E, 0x94, 0x9B, 0x1E, 0x87, 0xE9, 0xCE, 0x55, 0x28, 0xDF, 0x8C, 0xA1, 0x89, 0x0D, 0xBF, 0xE6, 0x42, 0x68, 0x41, 0x99, 0x2D, 0x0F, 0xB0, 0x54, 0xBB, 0x16 ]) def aes_labelize_ge_sr(plt_attack, byte, key, leakage): pt_ct = [row[byte] for row in plt_attack] key_byte = np.full(len(pt_ct), key[byte]) state = [int(x) ^ int(k) for x, k in zip(np.asarray(pt_ct[:]), key_byte)] intermediate_values = sbox[state] if leakage == \"HW\": return [bin(iv).count(\"1\") for iv in intermediate_values] else: return intermediate_values def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): leakage_model = aes_leakage_model[\"leakage_model\"] target_byte = aes_leakage_model[\"byte\"] key = param[\"key\"] nt = len(x_validation) nt_interval = int(key_rank_attack_traces / key_rank_report_interval) key_ranking_sum = np.zeros(nt_interval) # ---------------------------------------------------------------------------------------------------------# # compute labels for key hypothesis # ---------------------------------------------------------------------------------------------------------# labels_key_hypothesis = np.zeros((256, nt)) for key_byte_hypothesis in range(0, 256): key_h = bytearray.fromhex(key) key_h[target_byte] = key_byte_hypothesis labels_key_hypothesis[key_byte_hypothesis][:] = aes_labelize_ge_sr(plaintexts_validation, target_byte, key_h, leakage_model) good_key = [int(x) for x in bytearray.fromhex(key)][target_byte] # ---------------------------------------------------------------------------------------------------------# # predict output probabilities for shuffled test or validation set # ---------------------------------------------------------------------------------------------------------# output_probabilities = model.predict(x_validation) probabilities_kg_all_traces = np.zeros((nt, 256)) for index in range(nt): probabilities_kg_all_traces[index] = output_probabilities[index][ np.asarray([int(leakage[index]) for leakage in labels_key_hypothesis[:]]) # array with 256 leakage values (1 per key guess) ] for key_rank_execution in range(key_rank_executions): probabilities_kg_all_traces_shuffled = shuffle(probabilities_kg_all_traces, random_state=random.randint(0, 100000)) key_probabilities = np.zeros(256) kr_count = 0 for index in range(key_rank_attack_traces): key_probabilities += np.log(probabilities_kg_all_traces_shuffled[index] + 1e-36) key_probabilities_sorted = np.argsort(key_probabilities)[::-1] if (index + 1) % key_rank_report_interval == 0: key_ranking_good_key = list(key_probabilities_sorted).index(good_key) + 1 key_ranking_sum[kr_count] += key_ranking_good_key kr_count += 1 final_kr = key_ranking_sum[nt_interval - 1] print(\"KR run: {} | final Guessing Entropy for correct key ({}): {})\".format(key_rank_execution + 1, good_key, final_kr / (key_rank_execution + 1))) guessing_entropy = key_ranking_sum / key_rank_executions return guessing_entropy[nt_interval - 1] Step 2: Calling early stopping custom metrics from script The code below provides an example of how to call early stopping custom metrics from the main script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) early_stopping = { \"metrics\": { \"accuracy\": { \"direction\": \"max\", \"class\": \"custom.custom_metrics.accuracy\", \"parameters\": [] }, \"loss\": { \"direction\": \"min\", \"class\": \"custom.custom_metrics.loss\", \"parameters\": [] }, \"number_of_traces\": { \"direction\": \"min\", \"class\": \"custom.custom_metrics.number_of_traces\", \"parameters\": [] }, \"success_rate\": { \"direction\": \"max\", \"class\": \"custom.custom_metrics.success_rate\", \"parameters\": [] } } } aisy.run( early_stopping=early_stopping, key_rank_attack_traces=500 ) metrics_validation = aisy.get_metrics_validation() for metric in metrics_validation: print(\"{}: {}\".format(metric['metric'], metric['values'])) Returning a tuple (or list of values) in the custom metric In the metric examples above ( accuracy.py , loss.py and guessing_entropy.py ), each metric returns a single (float) value. It also possible to return a tuple or a list of values instead of a single value. Retrieving the early stopping metric values As in the example above, the method metrics_validation = aisy.get_metrics_validation() returns the list of defined early stopping metrics and their values for each epoch. Running the above code, the user should get a results similar to the following one: val_accuracy: [0.269, 0.288, 0.27, 0.248, 0.263, 0.248, 0.241, 0.248, 0.264, 0.232] val_loss: [1.7930998115539551, 1.7487744626998902, 1.7676906442642213, 1.7654361686706543, 1.7366435260772706, 1.7864224281311034, 1.7834029178619384, 1.8150662136077882, 1.8051397848129271, 1.8797064323425292] val_guessing_entropy: [203.0, 182.0, 94.0, 86.0, 104.0, 72.0, 82.0, 125.0, 141.0, 129.0] val_success_rate: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] Visualizing metric results in the Web Application It is also important to note that custom metric retuls are also stored in the defined SQLite database file. Plots can also be observed in the web application interface that runs on localhost . The plots can be seen as in the example below: For each early stopping metric, the framework computes the guessing entropy: In the above plot, the Attack label (in blue) denotes the guessing entropy of a single key byte computed after all processed epochs (end of training). The other lines refer to the guessing entropy at the epoch where the correspondent metric achieves the best value. The same type of result is provided for success rates:","title":"Custom Early Stopping Metrics"},{"location":"custom_metrics/#custom-early-stopping-metrics","text":"AISY Framework allows easy definition of custom metrics to be monitored during training. This is specially important when early stopping is considered in the profiled SCA. Custom metrics are processed in a standard callback, EarlyStoppingCallback . Therefore, an early_stopping dictionary is passed to the run method: early_stopping = { \"metrics\": { \"accuracy\": { \"direction\": \"max\", \"class\": \"accuracy\", \"parameters\": [] }, \"loss\": { \"direction\": \"min\", \"class\": \"loss\", \"parameters\": [] }, \"guessing_entropy\": { \"direction\": \"min\", \"class\": \"guessing_entropy\", \"parameters\": [] }, \"success_rate\": { \"direction\": \"max\", \"class\": \"success_rate\", \"parameters\": [] } } } In the example above, we have four custom metrics. Of course, these four metrics are already implemented as standard metrics in the AISY Framework. However, these standard metrics will only be used in the early stopping mechanism if included in the early_stopping dictionary. Note that for each metric we have three attributes: direction , class and parameters . The direction indicates what is the reference (min or max) to be detected as best achieved value. The class indicates the name of the python file containing the code for the metric and must be placed in custom/custom_metrics/ directory.","title":"Custom Early Stopping Metrics"},{"location":"custom_metrics/#step-1-creating-custom-metric-py-file","text":"Create a new .py file named with the attribute class as defined in early_stopping . Following the early_stopping dictionary from the above example, we create four python files inside custom/custom_metrics/ directory: accuracy.py loss.py guessing_entropy.py success_rate.py","title":"Step 1: Creating custom metric .py file"},{"location":"custom_metrics/#standard-custom-metric-file-structure-standard-parameters","text":"Inside each of this files, create the following basic method run : def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): pass where parameters are explained below x_profiling: profiling trace set. y_profiling: categorical profiling set labels. plaintexts_profiling: profiling set plaintext. ciphertexts_profiling: profiling set ciphertexts. key_profiling: profiling set key (can be a different key per trace). x_validation: validation trace set. y_validation: categorical validation set labels. plaintexts_validation: validation set plaintexts. ciphertexts_validation: validation set ciphertexts. key_validation: validation set key. x_attack: attack trace set. y_attack: categorical attack set labels. plaintexts_attack: attack set plaintexts. ciphertexts_attack: attack set ciphertexts. key_attack: attack set key. param: target parameteres. It is a dictionary as in the example below: param = { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 2000 } aes_leakage_model: leakage model parameters. It is a dictionary as in the example below: aes_leakage_model = { \"leakage_model\": \"HW\", # \"HW\", \"ID\" or \"bit\" \"byte\": byte, } key_rank_executions: number of key rank executions in the guessing entropy calculatioin. key_rank_report_interval: report trace interval in metric calculations (e.g., guessing entropy, success rate, etc.) key_rank_attack_traces: number of randomly selected traces from attack trace set to be used in each key rank calculation. model: keras (neural network) model. *args: list of additional and optional parameters to be passed with early_stopping dictionary.","title":"Standard custom metric file structure (standard parameters)"},{"location":"custom_metrics/#examples-of-custom-metric-files","text":"For accuracy.py metric, the code can be defined as in the example below: def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): loss, acc = model.evaluate(x_validation, y_validation, verbose=0) return acc For loss.py metric, the code can be defined as in the example below: def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): loss, _ = model.evaluate(x_validation, y_validation, verbose=0) return loss Below, we provide a full example for guessing_entropy.py where guessing entropy is calculated for the validation trace set having one byte of S-Box output of first AES encryption round as target state. The run method returns the guessing entropy after processing key_rank_attack_traces validation traces. import numpy as np import random from sklearn.utils import shuffle sbox = np.array([ 0x63, 0x7C, 0x77, 0x7B, 0xF2, 0x6B, 0x6F, 0xC5, 0x30, 0x01, 0x67, 0x2B, 0xFE, 0xD7, 0xAB, 0x76, 0xCA, 0x82, 0xC9, 0x7D, 0xFA, 0x59, 0x47, 0xF0, 0xAD, 0xD4, 0xA2, 0xAF, 0x9C, 0xA4, 0x72, 0xC0, 0xB7, 0xFD, 0x93, 0x26, 0x36, 0x3F, 0xF7, 0xCC, 0x34, 0xA5, 0xE5, 0xF1, 0x71, 0xD8, 0x31, 0x15, 0x04, 0xC7, 0x23, 0xC3, 0x18, 0x96, 0x05, 0x9A, 0x07, 0x12, 0x80, 0xE2, 0xEB, 0x27, 0xB2, 0x75, 0x09, 0x83, 0x2C, 0x1A, 0x1B, 0x6E, 0x5A, 0xA0, 0x52, 0x3B, 0xD6, 0xB3, 0x29, 0xE3, 0x2F, 0x84, 0x53, 0xD1, 0x00, 0xED, 0x20, 0xFC, 0xB1, 0x5B, 0x6A, 0xCB, 0xBE, 0x39, 0x4A, 0x4C, 0x58, 0xCF, 0xD0, 0xEF, 0xAA, 0xFB, 0x43, 0x4D, 0x33, 0x85, 0x45, 0xF9, 0x02, 0x7F, 0x50, 0x3C, 0x9F, 0xA8, 0x51, 0xA3, 0x40, 0x8F, 0x92, 0x9D, 0x38, 0xF5, 0xBC, 0xB6, 0xDA, 0x21, 0x10, 0xFF, 0xF3, 0xD2, 0xCD, 0x0C, 0x13, 0xEC, 0x5F, 0x97, 0x44, 0x17, 0xC4, 0xA7, 0x7E, 0x3D, 0x64, 0x5D, 0x19, 0x73, 0x60, 0x81, 0x4F, 0xDC, 0x22, 0x2A, 0x90, 0x88, 0x46, 0xEE, 0xB8, 0x14, 0xDE, 0x5E, 0x0B, 0xDB, 0xE0, 0x32, 0x3A, 0x0A, 0x49, 0x06, 0x24, 0x5C, 0xC2, 0xD3, 0xAC, 0x62, 0x91, 0x95, 0xE4, 0x79, 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65, 0x7A, 0xAE, 0x08, 0xBA, 0x78, 0x25, 0x2E, 0x1C, 0xA6, 0xB4, 0xC6, 0xE8, 0xDD, 0x74, 0x1F, 0x4B, 0xBD, 0x8B, 0x8A, 0x70, 0x3E, 0xB5, 0x66, 0x48, 0x03, 0xF6, 0x0E, 0x61, 0x35, 0x57, 0xB9, 0x86, 0xC1, 0x1D, 0x9E, 0xE1, 0xF8, 0x98, 0x11, 0x69, 0xD9, 0x8E, 0x94, 0x9B, 0x1E, 0x87, 0xE9, 0xCE, 0x55, 0x28, 0xDF, 0x8C, 0xA1, 0x89, 0x0D, 0xBF, 0xE6, 0x42, 0x68, 0x41, 0x99, 0x2D, 0x0F, 0xB0, 0x54, 0xBB, 0x16 ]) def aes_labelize_ge_sr(plt_attack, byte, key, leakage): pt_ct = [row[byte] for row in plt_attack] key_byte = np.full(len(pt_ct), key[byte]) state = [int(x) ^ int(k) for x, k in zip(np.asarray(pt_ct[:]), key_byte)] intermediate_values = sbox[state] if leakage == \"HW\": return [bin(iv).count(\"1\") for iv in intermediate_values] else: return intermediate_values def run(x_profiling, y_profiling, plaintexts_profiling, ciphertexts_profiling, key_profiling, x_validation, y_validation, plaintexts_validation, ciphertexts_validation, key_validation, x_attack, y_attack, plaintexts_attack, ciphertexts_attack, key_attack, param, aes_leakage_model, key_rank_executions, key_rank_report_interval, key_rank_attack_traces, model, *args): leakage_model = aes_leakage_model[\"leakage_model\"] target_byte = aes_leakage_model[\"byte\"] key = param[\"key\"] nt = len(x_validation) nt_interval = int(key_rank_attack_traces / key_rank_report_interval) key_ranking_sum = np.zeros(nt_interval) # ---------------------------------------------------------------------------------------------------------# # compute labels for key hypothesis # ---------------------------------------------------------------------------------------------------------# labels_key_hypothesis = np.zeros((256, nt)) for key_byte_hypothesis in range(0, 256): key_h = bytearray.fromhex(key) key_h[target_byte] = key_byte_hypothesis labels_key_hypothesis[key_byte_hypothesis][:] = aes_labelize_ge_sr(plaintexts_validation, target_byte, key_h, leakage_model) good_key = [int(x) for x in bytearray.fromhex(key)][target_byte] # ---------------------------------------------------------------------------------------------------------# # predict output probabilities for shuffled test or validation set # ---------------------------------------------------------------------------------------------------------# output_probabilities = model.predict(x_validation) probabilities_kg_all_traces = np.zeros((nt, 256)) for index in range(nt): probabilities_kg_all_traces[index] = output_probabilities[index][ np.asarray([int(leakage[index]) for leakage in labels_key_hypothesis[:]]) # array with 256 leakage values (1 per key guess) ] for key_rank_execution in range(key_rank_executions): probabilities_kg_all_traces_shuffled = shuffle(probabilities_kg_all_traces, random_state=random.randint(0, 100000)) key_probabilities = np.zeros(256) kr_count = 0 for index in range(key_rank_attack_traces): key_probabilities += np.log(probabilities_kg_all_traces_shuffled[index] + 1e-36) key_probabilities_sorted = np.argsort(key_probabilities)[::-1] if (index + 1) % key_rank_report_interval == 0: key_ranking_good_key = list(key_probabilities_sorted).index(good_key) + 1 key_ranking_sum[kr_count] += key_ranking_good_key kr_count += 1 final_kr = key_ranking_sum[nt_interval - 1] print(\"KR run: {} | final Guessing Entropy for correct key ({}): {})\".format(key_rank_execution + 1, good_key, final_kr / (key_rank_execution + 1))) guessing_entropy = key_ranking_sum / key_rank_executions return guessing_entropy[nt_interval - 1]","title":"Examples of custom metric files"},{"location":"custom_metrics/#step-2-calling-early-stopping-custom-metrics-from-script","text":"The code below provides an example of how to call early stopping custom metrics from the main script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) early_stopping = { \"metrics\": { \"accuracy\": { \"direction\": \"max\", \"class\": \"custom.custom_metrics.accuracy\", \"parameters\": [] }, \"loss\": { \"direction\": \"min\", \"class\": \"custom.custom_metrics.loss\", \"parameters\": [] }, \"number_of_traces\": { \"direction\": \"min\", \"class\": \"custom.custom_metrics.number_of_traces\", \"parameters\": [] }, \"success_rate\": { \"direction\": \"max\", \"class\": \"custom.custom_metrics.success_rate\", \"parameters\": [] } } } aisy.run( early_stopping=early_stopping, key_rank_attack_traces=500 ) metrics_validation = aisy.get_metrics_validation() for metric in metrics_validation: print(\"{}: {}\".format(metric['metric'], metric['values']))","title":"Step 2: Calling early stopping custom metrics from script"},{"location":"custom_metrics/#returning-a-tuple-or-list-of-values-in-the-custom-metric","text":"In the metric examples above ( accuracy.py , loss.py and guessing_entropy.py ), each metric returns a single (float) value. It also possible to return a tuple or a list of values instead of a single value.","title":"Returning a tuple (or list of values) in the custom metric"},{"location":"custom_metrics/#retrieving-the-early-stopping-metric-values","text":"As in the example above, the method metrics_validation = aisy.get_metrics_validation() returns the list of defined early stopping metrics and their values for each epoch. Running the above code, the user should get a results similar to the following one: val_accuracy: [0.269, 0.288, 0.27, 0.248, 0.263, 0.248, 0.241, 0.248, 0.264, 0.232] val_loss: [1.7930998115539551, 1.7487744626998902, 1.7676906442642213, 1.7654361686706543, 1.7366435260772706, 1.7864224281311034, 1.7834029178619384, 1.8150662136077882, 1.8051397848129271, 1.8797064323425292] val_guessing_entropy: [203.0, 182.0, 94.0, 86.0, 104.0, 72.0, 82.0, 125.0, 141.0, 129.0] val_success_rate: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]","title":"Retrieving the early stopping metric values"},{"location":"custom_metrics/#visualizing-metric-results-in-the-web-application","text":"It is also important to note that custom metric retuls are also stored in the defined SQLite database file. Plots can also be observed in the web application interface that runs on localhost . The plots can be seen as in the example below: For each early stopping metric, the framework computes the guessing entropy: In the above plot, the Attack label (in blue) denotes the guessing entropy of a single key byte computed after all processed epochs (end of training). The other lines refer to the guessing entropy at the epoch where the correspondent metric achieves the best value. The same type of result is provided for success rates:","title":"Visualizing metric results in the Web Application"},{"location":"data_augmentation/","text":"Data Augmentation AISY Framework also allows easy configuration of data augmentation techniques during model training. Data augmentation is common machine learning technique and it is widely applied in side-channel analysis in order to improve model learnability. Basically, data augmentation allows small modifications in side-channel traces during training, which makes the model to reduce overfitting and, as a consequence, to improve generalization. Creating Data Augmentation Methods THe user may define data augmentation methods in any location. For better code organization, we recommend to write the methods in the file custom/custom_data_augmentation/data_augmentation.py Code below provides two examples of data augmentation methods: data_augmentation_shifts and data_augmentation_gaussian_noise : import random import numpy as np def data_augmentation_shifts(data_set_samples, data_set_labels, batch_size, input_layer_shape): ns = len(data_set_samples[0]) while True: x_train_shifted = np.zeros((batch_size, ns)) rnd = random.randint(0, len(data_set_samples) - batch_size) x_mini_batch = data_set_samples[rnd:rnd + batch_size] for trace_index in range(batch_size): x_train_shifted[trace_index] = x_mini_batch[trace_index] shift = random.randint(-5, 5) if shift > 0: x_train_shifted[trace_index][0:ns - shift] = x_mini_batch[trace_index][shift:ns] x_train_shifted[trace_index][ns - shift:ns] = x_mini_batch[trace_index][0:shift] else: x_train_shifted[trace_index][0:abs(shift)] = x_mini_batch[trace_index][ns - abs(shift):ns] x_train_shifted[trace_index][abs(shift):ns] = x_mini_batch[trace_index][0:ns - abs(shift)] if len(input_layer_shape) == 3: x_train_shifted_reshaped = x_train_shifted.reshape((x_train_shifted.shape[0], x_train_shifted.shape[1], 1)) yield x_train_shifted_reshaped, data_set_labels[rnd:rnd + batch_size] else: yield x_train_shifted, data_set_labels[rnd:rnd + batch_size] def data_augmentation_gaussian_noise(data_set_samples, data_set_labels, batch_size, input_layer_shape): ns = len(data_set_samples[0]) while True: x_train_augmented = np.zeros((batch_size, ns)) rnd = random.randint(0, len(data_set_samples) - batch_size) x_mini_batch = data_set_samples[rnd:rnd + batch_size] noise = np.random.normal(0, 1, ns) for trace_index in range(batch_size): x_train_augmented[trace_index] = x_mini_batch[trace_index] + noise if len(input_layer_shape) == 3: x_train_augmented_reshaped = x_train_augmented.reshape((x_train_augmented.shape[0], x_train_augmented.shape[1], 1)) yield x_train_augmented_reshaped, data_set_labels[rnd:rnd + batch_size] else: yield x_train_augmented, data_set_labels[rnd:rnd + batch_size] As we can see, the method must be created with four input parameters: data_set_samples: : array containing the profiling traces. data_set_labels: : array containing the profiling categorical labels. batch_size: : integer defining the batch size. input_layer_shape: : shape of the input layer. When data augmentation is considered, the model training is done with fit_generator method from Keras. Calling Data Augmentation in the Main Script To pass data augmentation to the run method, user must enter two parameter in a list, as in the example below: import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_data_augmentation.data_augmentation import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) aisy.run(data_augmentation=[data_augmentation_gaussian_noise, 100]) The first parameter indicates the name of the data augmentation method, as defined in the custom/custom_data_augmentation/data_augmentation.py file. The second parameter indicates the number of data augmentation iterarions in a single epochs. Each iteration will apply data augmentation method to a defined amount of profiling traces. In our examples above, each iteration randomly select a number of profiling traces equivalent to the batch size and applies the modifications.","title":"Data Augmentation"},{"location":"data_augmentation/#data-augmentation","text":"AISY Framework also allows easy configuration of data augmentation techniques during model training. Data augmentation is common machine learning technique and it is widely applied in side-channel analysis in order to improve model learnability. Basically, data augmentation allows small modifications in side-channel traces during training, which makes the model to reduce overfitting and, as a consequence, to improve generalization.","title":"Data Augmentation"},{"location":"data_augmentation/#creating-data-augmentation-methods","text":"THe user may define data augmentation methods in any location. For better code organization, we recommend to write the methods in the file custom/custom_data_augmentation/data_augmentation.py Code below provides two examples of data augmentation methods: data_augmentation_shifts and data_augmentation_gaussian_noise : import random import numpy as np def data_augmentation_shifts(data_set_samples, data_set_labels, batch_size, input_layer_shape): ns = len(data_set_samples[0]) while True: x_train_shifted = np.zeros((batch_size, ns)) rnd = random.randint(0, len(data_set_samples) - batch_size) x_mini_batch = data_set_samples[rnd:rnd + batch_size] for trace_index in range(batch_size): x_train_shifted[trace_index] = x_mini_batch[trace_index] shift = random.randint(-5, 5) if shift > 0: x_train_shifted[trace_index][0:ns - shift] = x_mini_batch[trace_index][shift:ns] x_train_shifted[trace_index][ns - shift:ns] = x_mini_batch[trace_index][0:shift] else: x_train_shifted[trace_index][0:abs(shift)] = x_mini_batch[trace_index][ns - abs(shift):ns] x_train_shifted[trace_index][abs(shift):ns] = x_mini_batch[trace_index][0:ns - abs(shift)] if len(input_layer_shape) == 3: x_train_shifted_reshaped = x_train_shifted.reshape((x_train_shifted.shape[0], x_train_shifted.shape[1], 1)) yield x_train_shifted_reshaped, data_set_labels[rnd:rnd + batch_size] else: yield x_train_shifted, data_set_labels[rnd:rnd + batch_size] def data_augmentation_gaussian_noise(data_set_samples, data_set_labels, batch_size, input_layer_shape): ns = len(data_set_samples[0]) while True: x_train_augmented = np.zeros((batch_size, ns)) rnd = random.randint(0, len(data_set_samples) - batch_size) x_mini_batch = data_set_samples[rnd:rnd + batch_size] noise = np.random.normal(0, 1, ns) for trace_index in range(batch_size): x_train_augmented[trace_index] = x_mini_batch[trace_index] + noise if len(input_layer_shape) == 3: x_train_augmented_reshaped = x_train_augmented.reshape((x_train_augmented.shape[0], x_train_augmented.shape[1], 1)) yield x_train_augmented_reshaped, data_set_labels[rnd:rnd + batch_size] else: yield x_train_augmented, data_set_labels[rnd:rnd + batch_size] As we can see, the method must be created with four input parameters: data_set_samples: : array containing the profiling traces. data_set_labels: : array containing the profiling categorical labels. batch_size: : integer defining the batch size. input_layer_shape: : shape of the input layer. When data augmentation is considered, the model training is done with fit_generator method from Keras.","title":"Creating Data Augmentation Methods"},{"location":"data_augmentation/#calling-data-augmentation-in-the-main-script","text":"To pass data augmentation to the run method, user must enter two parameter in a list, as in the example below: import aisy_sca from app import * from custom.custom_models.neural_networks import * from custom.custom_data_augmentation.data_augmentation import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) aisy.set_neural_network(mlp) aisy.run(data_augmentation=[data_augmentation_gaussian_noise, 100]) The first parameter indicates the name of the data augmentation method, as defined in the custom/custom_data_augmentation/data_augmentation.py file. The second parameter indicates the number of data augmentation iterarions in a single epochs. Each iteration will apply data augmentation method to a defined amount of profiling traces. In our examples above, each iteration randomly select a number of profiling traces equivalent to the batch size and applies the modifications.","title":"Calling Data Augmentation in the Main Script"},{"location":"databases/","text":"Databases The AISY database package must be installed together with the framework files. In requirements.txt , python package aisy-database aisy-database 0.2.0 is already set. Database root folder The root location of databases can be defined in two ways. 1. By defining database location in app.py file The root location of sqlite database is defined in app.py file as follows: databases_root_folder = \"my_location/databases/\" The user is free to change this location. 2. By defining database location in script file The user can also easily set the database location in the main script, as in the example below: aisy = aisy_sca.Aisy() aisy.set_database_root_folder(\"my_location/\") aisy.set_database_name(\"my_database.sqlite\") AISY framework makes use of SQlite database. In the main script, the user enter the name of the .sqlite database file. If file already exists, then new entries from the current analysis are stored for the existing tables. Otherwise, the .sqlite file is created and stored in resources/databases folder. Note that resources folder is only generated once the first analysis is executed. Tables Database code is implemented from open-source SQLAlchemy python package. Here we provide a list of tables ( and the column types from SQLAlchemy package) that are generate in each .sqlite file. Analysis id (Integer): primary key id for entry. datetime (DateTime): data and time for the entry. db_filename (String): the name of sqlite file. dataset (String): the name of evaluated SCA dataset. settings (JSON): dictionary of analysis settings. elapsed_time (Float): the elapsed time for the analysis. deleted (Boolean): indicates if entry is deleted by the user. HyperParameter id (Integer): primary key id for entry. hyper_parameters (JSON): dictionary containing list of hyperparameters. analysis_id (Integer, ForeignKey): id from Analysis Table. NeuralNetwork id (Integer): primary key id for entry. model_name (String): name of neural network model. description (String): description of neural network model. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. LeakageModel id (Integer): primary key id for entry. leakage_model (JSON): dictionary containing list of leakage model attributes. analysis_id (Integer, ForeignKey): id from Analysis Table. Metric id (Integer): primary key id for entry. values (JSON): value of the metric. label (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table. GuessingEntropy id (Integer): primary key id for entry. values (JSON): list of values. label (String): name of the guessing entropy result. report_interval (Intger): report interval (step in the number of side channel traces or measurements) which key rank is stored. metric (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table. SuccessRate id (Integer): primary key id for entry. values (JSON): list of values. label (String): name of the success rate result. report_interval (Intger): report interval (step in the number of side channel traces or measurements) which success rate is stored. metric (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table. Visualization id (Integer): primary key id for entry. values (JSON): list of values. epoch (Integer): epoch value for visualization results. label (String): metric label. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. HyperparameterMetric id (Integer): primary key id for entry. metric_id (Integer, ForeignKey): id from Metric Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. HyperParameterSuccessRate id (Integer): primary key id for entry. success_rate_id (Integer, ForeignKey): id from SuccessRate Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. HyperParameterGuessingEntropy id (Integer): primary key id for entry. guessing_entropy_id (Integer, ForeignKey): id from SuccessRate Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. ConfusionMatrix id (Integer): primary key id for entry. y_pred (JSON): list of y_pred values. y_true (Integer): index of y_true label. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table. ProbabilityRank id (Integer): primary key id for entry. ranks (JSON): list of ranks values (one row per key guess ). classes (Integer): number of classes in the classification. correct_key_byte (Integer): value of correct key byte. key_guess (Integer): value of key guess. analysis_id (Integer, ForeignKey): id from Analysis Table. RandomStatesHyperparameter id (Integer): primary key id for entry. random_states (JSON) : list of random states label (String): random state label. index (Integer : random state index. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"Databases"},{"location":"databases/#databases","text":"The AISY database package must be installed together with the framework files. In requirements.txt , python package aisy-database aisy-database 0.2.0 is already set.","title":"Databases"},{"location":"databases/#database-root-folder","text":"The root location of databases can be defined in two ways.","title":"Database root folder"},{"location":"databases/#1-by-defining-database-location-inapppy-file","text":"The root location of sqlite database is defined in app.py file as follows: databases_root_folder = \"my_location/databases/\" The user is free to change this location.","title":"1. By defining database location inapp.py file"},{"location":"databases/#2-by-defining-database-location-in-script-file","text":"The user can also easily set the database location in the main script, as in the example below: aisy = aisy_sca.Aisy() aisy.set_database_root_folder(\"my_location/\") aisy.set_database_name(\"my_database.sqlite\") AISY framework makes use of SQlite database. In the main script, the user enter the name of the .sqlite database file. If file already exists, then new entries from the current analysis are stored for the existing tables. Otherwise, the .sqlite file is created and stored in resources/databases folder. Note that resources folder is only generated once the first analysis is executed.","title":"2. By defining database location in script file"},{"location":"databases/#tables","text":"Database code is implemented from open-source SQLAlchemy python package. Here we provide a list of tables ( and the column types from SQLAlchemy package) that are generate in each .sqlite file.","title":"Tables"},{"location":"databases/#analysis","text":"id (Integer): primary key id for entry. datetime (DateTime): data and time for the entry. db_filename (String): the name of sqlite file. dataset (String): the name of evaluated SCA dataset. settings (JSON): dictionary of analysis settings. elapsed_time (Float): the elapsed time for the analysis. deleted (Boolean): indicates if entry is deleted by the user.","title":"Analysis"},{"location":"databases/#hyperparameter","text":"id (Integer): primary key id for entry. hyper_parameters (JSON): dictionary containing list of hyperparameters. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"HyperParameter"},{"location":"databases/#neuralnetwork","text":"id (Integer): primary key id for entry. model_name (String): name of neural network model. description (String): description of neural network model. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"NeuralNetwork"},{"location":"databases/#leakagemodel","text":"id (Integer): primary key id for entry. leakage_model (JSON): dictionary containing list of leakage model attributes. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"LeakageModel"},{"location":"databases/#metric","text":"id (Integer): primary key id for entry. values (JSON): value of the metric. label (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"Metric"},{"location":"databases/#guessingentropy","text":"id (Integer): primary key id for entry. values (JSON): list of values. label (String): name of the guessing entropy result. report_interval (Intger): report interval (step in the number of side channel traces or measurements) which key rank is stored. metric (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"GuessingEntropy"},{"location":"databases/#successrate","text":"id (Integer): primary key id for entry. values (JSON): list of values. label (String): name of the success rate result. report_interval (Intger): report interval (step in the number of side channel traces or measurements) which success rate is stored. metric (String): name of the metric. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"SuccessRate"},{"location":"databases/#visualization","text":"id (Integer): primary key id for entry. values (JSON): list of values. epoch (Integer): epoch value for visualization results. label (String): metric label. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"Visualization"},{"location":"databases/#hyperparametermetric","text":"id (Integer): primary key id for entry. metric_id (Integer, ForeignKey): id from Metric Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"HyperparameterMetric"},{"location":"databases/#hyperparametersuccessrate","text":"id (Integer): primary key id for entry. success_rate_id (Integer, ForeignKey): id from SuccessRate Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"HyperParameterSuccessRate"},{"location":"databases/#hyperparameterguessingentropy","text":"id (Integer): primary key id for entry. guessing_entropy_id (Integer, ForeignKey): id from SuccessRate Table. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"HyperParameterGuessingEntropy"},{"location":"databases/#confusionmatrix","text":"id (Integer): primary key id for entry. y_pred (JSON): list of y_pred values. y_true (Integer): index of y_true label. hyperparameters_id (Integer, ForeignKey): id from Hyperparameters Table. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"ConfusionMatrix"},{"location":"databases/#probabilityrank","text":"id (Integer): primary key id for entry. ranks (JSON): list of ranks values (one row per key guess ). classes (Integer): number of classes in the classification. correct_key_byte (Integer): value of correct key byte. key_guess (Integer): value of key guess. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"ProbabilityRank"},{"location":"databases/#randomstateshyperparameter","text":"id (Integer): primary key id for entry. random_states (JSON) : list of random states label (String): random state label. index (Integer : random state index. analysis_id (Integer, ForeignKey): id from Analysis Table.","title":"RandomStatesHyperparameter"},{"location":"datasets/","text":"Datasets Open-source AES datasets The standard version of AISY Framework comes with the definitions of open-source side-channel analysis datasets. ASCAD Fixed Key ASCAD Random Keys AES_HD AES_HD_ext The user can download the datasets by running the following commands in a terminal: Windows 10 curl.exe -o ASCAD_data.zip https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip curl.exe -o ascad-variable.h5 https://static.data.gouv.fr/resources/ascad-atmega-8515-variable-key/20190903-083349/ascad-variable.h5 curl.exe -o ches_ctf.h5 http://aisylabdatasets.ewi.tudelft.nl/aes_hd.h5 curl.exe -o ches_ctf.h5 http://aisylabdatasets.ewi.tudelft.nl/aes_hd_ext.h5 Linux wget https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip wget https://static.data.gouv.fr/resources/ascad-atmega-8515-variable-key/20190903-083349/ascad-variable.h5 wget http://aisylabdatasets.ewi.tudelft.nl/aes_hd.h5 wget http://aisylabdatasets.ewi.tudelft.nl/aes_hd_ext.h5 Creating datasets for AISY Framework .h5 datasets In the academic community, the most used datasets in deep learning-based SCA are ASCAD trace sets from ANSSI (France) github repository . The authors of ASCAD repository delivered source code to generate .h5 datasets according to their specific format. The lines 127-145 from file ASCAD_generate.py provide an example of how to generate .h5 datasets. If the user wants to process datasets in the .h5 format, AISY Framework expects files generated according to ASCAD database. The only limitation found in ASCAD_generate.py code is that it does not generate ciphertexts in the metadata field. Therefore, we modified the code in order to overcome this limitation. out_file = h5py.File('new_dataset.h5', 'w') profiling_traces_group = out_file.create_group(\"Profiling_traces\") attack_traces_group = out_file.create_group(\"Attack_traces\") profiling_traces_group.create_dataset(name=\"traces\", data=train_samples, dtype=train_samples.dtype) attack_traces_group.create_dataset(name=\"traces\", data=test_samples, dtype=test_samples.dtype) metadata_type_profiling = np.dtype([(\"plaintext\", profiling_plaintext.dtype, (len(profiling_plaintext[0]),)), (\"ciphertext\", profiling_ciphertext.dtype, (len(profiling_ciphertext[0]),)), (\"key\", profiling_key.dtype, (len(profiling_key[0]),)), (\"mask\", profiling_key.dtype, (len(profiling_key[0]),)) ]) metadata_type_attack = np.dtype([(\"plaintext\", attack_plaintext.dtype, (len(attack_plaintext[0]),)), (\"ciphertext\", attack_ciphertext.dtype, (len(attack_ciphertext[0]),)), (\"key\", attack_key.dtype, (len(attack_key[0]),)), (\"mask\", attack_key.dtype, (len(attack_key[0]),)) ]) profiling_metadata = np.array([(profiling_plaintext[n], profiling_ciphertext[n], profiling_key[n], profiling_mask[n]) for n, k in zip(profiling_index, range(0, len(train_samples)))], dtype=metadata_type_profiling) profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type_profiling) attack_metadata = np.array([(attack_plaintext[n], attack_ciphertext[n], attack_key[n], attack_mask[n]) for n, k in zip(attack_index, range(0, len(test_samples)))], dtype=metadata_type_attack) attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type_attack) out_file.flush() out_file.close() Here is an example of how to read .h5 dataset in the ASCAD format: in_file = h5py.File(\"my_location/my_dataset.h5\", \"r\") # reading trace samples profiling_samples = numpy.array(in_file['Profiling_traces/traces'], dtype=numpy.float64) attack_samples = numpy.array(in_file['Attack_traces/traces'], dtype=numpy.float64) # reading trace plaintexts profiling_plaintext = in_file['Profiling_traces/metadata']['plaintext'] attack_plaintext = in_file['Attack_traces/metadata']['plaintext'] # reading trace ciphertexts profiling_ciphertext = in_file['Profiling_traces/metadata']['ciphertext'] attack_ciphertext = in_file['Attack_traces/metadata']['ciphertext'] # reading trace keys profiling_key = in_file['Profiling_traces/metadata']['key'] attack_key = in_file['Attack_traces/metadata']['key'] # reading trace masks profiling_mask = in_file['Profiling_traces/metadata']['mask'] attack_mask = in_file['Attack_traces/metadata']['mask'] Other dataset formats .npz and .csv dataset formats will be available in future releases. Dataset root folder The root location of datasets can be defined in two ways. Defining datasets location in app.py file The root location of datasets is defined in app.py file as follows: datasets_root_folder = \"my_location/datasets/\" The user is free to change this location. Defining database location in script file The user can also easily set the dataset location in the main script, as in the example below: aisy = AisyAes() aisy.set_datasets_root_folder(\"my_location/datasets/\") aisy.set_dataset(\"ascad-variable.h5\") Datasets Specifications As for the location, specifications for the datasets can be done either in the main script or, as recommended, in a dictionary containing six specifications for all datasets in custom/custom_datasets/datasets.py file. The possible specifications are: \"file_name\": string setting the file name of the dataset. Example: \"filename\": \"ascad-variable.h5\" . \"key\": hex string defining the dataset key. Example: \"key\": \"00112233445566778899AABBCCDDEEFF\" . \"first_sample\": integer defining the index of the first sample in each trace to be analysed. Example: \"first_sample\": 0 . \"number_of_samples\": integer defining the number of samples in each trace to be analysed. Example: \"number_of_samples\": 700 . \"number_of_profiling_traces\": integer defining the number of profiling traces. Example: \"number_of_profiling_traces\": 200000 . \"number_of_attack_traces\": integer defining the number of attack traces. Example: \"number_of_attack_traces\": 10000 . If the user decides to set the dataset specification in custom/custom_datasets/datasets.py file, the dataset is called from the main script according to the name defined in the dictionary. In the main script: aisy = AisyAes() aisy.set_dataset(\"ascad-variable.h5\") And in the dictionary in custom/custom_datasets/datasets.py : datasets_dict = { \"ascad-variable.h5\": { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 1000 } } Alternatively, the user can completely ignore custom/custom_datasets/datasets.py file and set all the specifications in the main script: aisy = aisy_sca.Aisy() aisy.set_dataset_filename(\"ascad-variable.h5\") aisy.set_key(\"00112233445566778899AABBCCDDEEFF\") aisy.set_number_of_profiling_traces(200000) aisy.set_number_of_attack_traces(10000) aisy.set_first_sample(0) aisy.set_number_of_samples(1400) Note that if the user decides to avoid the datasets definitions from datasets_dict , it is strictly necessary to provide all six specifications.","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#open-source-aes-datasets","text":"The standard version of AISY Framework comes with the definitions of open-source side-channel analysis datasets. ASCAD Fixed Key ASCAD Random Keys AES_HD AES_HD_ext The user can download the datasets by running the following commands in a terminal:","title":"Open-source AES datasets"},{"location":"datasets/#windows-10","text":"curl.exe -o ASCAD_data.zip https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip curl.exe -o ascad-variable.h5 https://static.data.gouv.fr/resources/ascad-atmega-8515-variable-key/20190903-083349/ascad-variable.h5 curl.exe -o ches_ctf.h5 http://aisylabdatasets.ewi.tudelft.nl/aes_hd.h5 curl.exe -o ches_ctf.h5 http://aisylabdatasets.ewi.tudelft.nl/aes_hd_ext.h5","title":"Windows 10"},{"location":"datasets/#linux","text":"wget https://www.data.gouv.fr/s/resources/ascad/20180530-163000/ASCAD_data.zip wget https://static.data.gouv.fr/resources/ascad-atmega-8515-variable-key/20190903-083349/ascad-variable.h5 wget http://aisylabdatasets.ewi.tudelft.nl/aes_hd.h5 wget http://aisylabdatasets.ewi.tudelft.nl/aes_hd_ext.h5","title":"Linux"},{"location":"datasets/#creating-datasets-for-aisy-framework","text":"","title":"Creating datasets for AISY Framework"},{"location":"datasets/#h5-datasets","text":"In the academic community, the most used datasets in deep learning-based SCA are ASCAD trace sets from ANSSI (France) github repository . The authors of ASCAD repository delivered source code to generate .h5 datasets according to their specific format. The lines 127-145 from file ASCAD_generate.py provide an example of how to generate .h5 datasets. If the user wants to process datasets in the .h5 format, AISY Framework expects files generated according to ASCAD database. The only limitation found in ASCAD_generate.py code is that it does not generate ciphertexts in the metadata field. Therefore, we modified the code in order to overcome this limitation. out_file = h5py.File('new_dataset.h5', 'w') profiling_traces_group = out_file.create_group(\"Profiling_traces\") attack_traces_group = out_file.create_group(\"Attack_traces\") profiling_traces_group.create_dataset(name=\"traces\", data=train_samples, dtype=train_samples.dtype) attack_traces_group.create_dataset(name=\"traces\", data=test_samples, dtype=test_samples.dtype) metadata_type_profiling = np.dtype([(\"plaintext\", profiling_plaintext.dtype, (len(profiling_plaintext[0]),)), (\"ciphertext\", profiling_ciphertext.dtype, (len(profiling_ciphertext[0]),)), (\"key\", profiling_key.dtype, (len(profiling_key[0]),)), (\"mask\", profiling_key.dtype, (len(profiling_key[0]),)) ]) metadata_type_attack = np.dtype([(\"plaintext\", attack_plaintext.dtype, (len(attack_plaintext[0]),)), (\"ciphertext\", attack_ciphertext.dtype, (len(attack_ciphertext[0]),)), (\"key\", attack_key.dtype, (len(attack_key[0]),)), (\"mask\", attack_key.dtype, (len(attack_key[0]),)) ]) profiling_metadata = np.array([(profiling_plaintext[n], profiling_ciphertext[n], profiling_key[n], profiling_mask[n]) for n, k in zip(profiling_index, range(0, len(train_samples)))], dtype=metadata_type_profiling) profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type_profiling) attack_metadata = np.array([(attack_plaintext[n], attack_ciphertext[n], attack_key[n], attack_mask[n]) for n, k in zip(attack_index, range(0, len(test_samples)))], dtype=metadata_type_attack) attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type_attack) out_file.flush() out_file.close() Here is an example of how to read .h5 dataset in the ASCAD format: in_file = h5py.File(\"my_location/my_dataset.h5\", \"r\") # reading trace samples profiling_samples = numpy.array(in_file['Profiling_traces/traces'], dtype=numpy.float64) attack_samples = numpy.array(in_file['Attack_traces/traces'], dtype=numpy.float64) # reading trace plaintexts profiling_plaintext = in_file['Profiling_traces/metadata']['plaintext'] attack_plaintext = in_file['Attack_traces/metadata']['plaintext'] # reading trace ciphertexts profiling_ciphertext = in_file['Profiling_traces/metadata']['ciphertext'] attack_ciphertext = in_file['Attack_traces/metadata']['ciphertext'] # reading trace keys profiling_key = in_file['Profiling_traces/metadata']['key'] attack_key = in_file['Attack_traces/metadata']['key'] # reading trace masks profiling_mask = in_file['Profiling_traces/metadata']['mask'] attack_mask = in_file['Attack_traces/metadata']['mask']","title":".h5 datasets"},{"location":"datasets/#other-dataset-formats","text":".npz and .csv dataset formats will be available in future releases.","title":"Other dataset formats"},{"location":"datasets/#dataset-root-folder","text":"The root location of datasets can be defined in two ways.","title":"Dataset root folder"},{"location":"datasets/#defining-datasets-location-inapppy-file","text":"The root location of datasets is defined in app.py file as follows: datasets_root_folder = \"my_location/datasets/\" The user is free to change this location.","title":"Defining datasets location inapp.py file"},{"location":"datasets/#defining-database-location-in-script-file","text":"The user can also easily set the dataset location in the main script, as in the example below: aisy = AisyAes() aisy.set_datasets_root_folder(\"my_location/datasets/\") aisy.set_dataset(\"ascad-variable.h5\")","title":"Defining database location in script file"},{"location":"datasets/#datasets-specifications","text":"As for the location, specifications for the datasets can be done either in the main script or, as recommended, in a dictionary containing six specifications for all datasets in custom/custom_datasets/datasets.py file. The possible specifications are: \"file_name\": string setting the file name of the dataset. Example: \"filename\": \"ascad-variable.h5\" . \"key\": hex string defining the dataset key. Example: \"key\": \"00112233445566778899AABBCCDDEEFF\" . \"first_sample\": integer defining the index of the first sample in each trace to be analysed. Example: \"first_sample\": 0 . \"number_of_samples\": integer defining the number of samples in each trace to be analysed. Example: \"number_of_samples\": 700 . \"number_of_profiling_traces\": integer defining the number of profiling traces. Example: \"number_of_profiling_traces\": 200000 . \"number_of_attack_traces\": integer defining the number of attack traces. Example: \"number_of_attack_traces\": 10000 . If the user decides to set the dataset specification in custom/custom_datasets/datasets.py file, the dataset is called from the main script according to the name defined in the dictionary. In the main script: aisy = AisyAes() aisy.set_dataset(\"ascad-variable.h5\") And in the dictionary in custom/custom_datasets/datasets.py : datasets_dict = { \"ascad-variable.h5\": { \"filename\": \"ascad-variable.h5\", \"key\": \"00112233445566778899AABBCCDDEEFF\", \"first_sample\": 0, \"number_of_samples\": 1400, \"number_of_profiling_traces\": 100000, \"number_of_attack_traces\": 1000 } } Alternatively, the user can completely ignore custom/custom_datasets/datasets.py file and set all the specifications in the main script: aisy = aisy_sca.Aisy() aisy.set_dataset_filename(\"ascad-variable.h5\") aisy.set_key(\"00112233445566778899AABBCCDDEEFF\") aisy.set_number_of_profiling_traces(200000) aisy.set_number_of_attack_traces(10000) aisy.set_first_sample(0) aisy.set_number_of_samples(1400) Note that if the user decides to avoid the datasets definitions from datasets_dict , it is strictly necessary to provide all six specifications.","title":"Datasets Specifications"},{"location":"ensembles/","text":"Ensembles When a specific deep learning model is not able to provide strong enough results in a given task, the ensemble of multiple models may be able to boost the performance of a profiled side-channel attack scenario. The ensemble feature provided in the AISY framework combines the prediction (output class probabilities) of multiple deep learning models. To use the feature, the user must define hyperparemeter search (random search or grid search). This way, the framework return the ensemble prediction of all searched models as well as the ensemble results for N best models. For more information about ensembles in profiled SCA, please refer to this paper . Validation set for ensembles As explained above, when ensemble are considered in the AISY framework, the user set what is the minimal number of models to ensemble in order to produce metric results based on the ensemble of those models. This way, the framework will need a validation set (which is different from the attack set ) to validate all search models and select the N best ones. Therefore, the amount of traces set as attack traces will be split into two halves, where the first half will be used as the validation set and the second half will be used as the attack set. Example Code import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"mlp\", \"hyper_parameters_search\": { 'neurons': [100, 200], 'layers': [2, 3], 'learning_rate': [0.001, 0.0001], 'activation': [\"relu\", \"selu\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( key_rank_attack_traces=500, grid_search=grid_search, ensemble=[10] )","title":"Ensembles"},{"location":"ensembles/#ensembles","text":"When a specific deep learning model is not able to provide strong enough results in a given task, the ensemble of multiple models may be able to boost the performance of a profiled side-channel attack scenario. The ensemble feature provided in the AISY framework combines the prediction (output class probabilities) of multiple deep learning models. To use the feature, the user must define hyperparemeter search (random search or grid search). This way, the framework return the ensemble prediction of all searched models as well as the ensemble results for N best models. For more information about ensembles in profiled SCA, please refer to this paper .","title":"Ensembles"},{"location":"ensembles/#validation-set-for-ensembles","text":"As explained above, when ensemble are considered in the AISY framework, the user set what is the minimal number of models to ensemble in order to produce metric results based on the ensemble of those models. This way, the framework will need a validation set (which is different from the attack set ) to validate all search models and select the N best ones. Therefore, the amount of traces set as attack traces will be split into two halves, where the first half will be used as the validation set and the second half will be used as the attack set.","title":"Validation set for ensembles"},{"location":"ensembles/#example-code","text":"import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"mlp\", \"hyper_parameters_search\": { 'neurons': [100, 200], 'layers': [2, 3], 'learning_rate': [0.001, 0.0001], 'activation': [\"relu\", \"selu\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( key_rank_attack_traces=500, grid_search=grid_search, ensemble=[10] )","title":"Example Code"},{"location":"errors/","text":"Errors Error 1: neural network model is not defined. ERROR 2: Dataset 'filename' not specified. Please use set_filename method to specify it. ERROR 3: Dataset 'key' not specified. Please use set_key method to specify it. ERROR 4 : Dataset 'first_sample' not specified. Please use set_first_sample method to specify it. ERROR 5: Dataset 'number_of_samples' not specified. Please use set_number_of_samples method to specify it. ERROR 6: Dataset 'number_of_profiling_traces' not specified. Please use set_number_of_profiling_traces method to specify it. ERROR 7: Dataset 'number_of_attack_traces' not specified. Please use set_number_of_attack_traces method to specify it. ERROR 8: ensemble feature requires the 'number_of_attack_traces' >= 2 x key_rank_attack_traces. ERROR 9: early stopping feature requires the 'number_of_attack_traces' >= 2 x key_rank_attack_traces. ERROR 10: Dataset format not supported. ERROR 11: when grid search has a stop condition, ensembles can't be applied. ERROR 12: number of models for ensembles can't be larger than maximum number of trials in random search. ERROR 13: when random search has a stop condition, ensembles can't be applied. ERROR 14: grid search feature requires the 'number_of_attack_traces' >= 2 x key_rank_attack_traces. ERROR 15: random search feature requires the 'number_of_attack_traces' >= 2 x key_rank_attack_traces. ERROR 16: to be able to reproduce analysis results, user must set random states for GE and SR with set_ge_sr_random_states() method ERROR 17: to be able to reproduce analysis results, user must set random states for ensembles for GE and SR with set_ge_sr_random_states_ensembles() method ERROR 18: to be able to reproduce analysis results, user must set random states for hyper-parameters search for GE and SR with set_ge_sr_random_states_search() method ERROR 19: to be able to reproduce analysis results, user must set random states for early stopping for GE and SR with set_ge_sr_random_states_early_stopping() method","title":"Errors"},{"location":"errors/#errors","text":"","title":"Errors"},{"location":"errors/#error-1-neural-network-model-is-not-defined","text":"","title":"Error 1: neural network model is not defined."},{"location":"errors/#error-2-dataset-filename-not-specified-please-use-set_filename-method-to-specify-it","text":"","title":"ERROR 2: Dataset 'filename' not specified. Please use set_filename method to specify it."},{"location":"errors/#error-3-dataset-key-not-specified-please-use-set_key-method-to-specify-it","text":"","title":"ERROR 3: Dataset 'key' not specified. Please use set_key method to specify it."},{"location":"errors/#error-4-dataset-first_sample-not-specified-please-use-set_first_sample-method-to-specify-it","text":"","title":"ERROR 4 : Dataset 'first_sample' not specified. Please use set_first_sample method to specify it."},{"location":"errors/#error-5-dataset-number_of_samples-not-specified-please-use-set_number_of_samples-method-to-specify-it","text":"","title":"ERROR 5: Dataset 'number_of_samples' not specified. Please use set_number_of_samples method to specify it."},{"location":"errors/#error-6-dataset-number_of_profiling_traces-not-specified-please-use-set_number_of_profiling_traces-method-to-specify-it","text":"","title":"ERROR 6: Dataset 'number_of_profiling_traces' not specified. Please use set_number_of_profiling_traces method to specify it."},{"location":"errors/#error-7-dataset-number_of_attack_traces-not-specified-please-use-set_number_of_attack_traces-method-to-specify-it","text":"","title":"ERROR 7: Dataset 'number_of_attack_traces' not specified. Please use set_number_of_attack_traces method to specify it."},{"location":"errors/#error-8-ensemble-feature-requires-the-number_of_attack_traces-2-x-key_rank_attack_traces","text":"","title":"ERROR 8: ensemble feature requires the 'number_of_attack_traces' &gt;= 2 x key_rank_attack_traces."},{"location":"errors/#error-9-early-stopping-feature-requires-the-number_of_attack_traces-2-x-key_rank_attack_traces","text":"","title":"ERROR 9: early stopping feature requires the 'number_of_attack_traces' &gt;= 2 x key_rank_attack_traces."},{"location":"errors/#error-10-dataset-format-not-supported","text":"","title":"ERROR 10: Dataset format not supported."},{"location":"errors/#error-11-when-grid-search-has-a-stop-condition-ensembles-cant-be-applied","text":"","title":"ERROR 11: when grid search has a stop condition, ensembles can't be applied."},{"location":"errors/#error-12-number-of-models-for-ensembles-cant-be-larger-than-maximum-number-of-trials-in-random-search","text":"","title":"ERROR 12: number of models for ensembles can't be larger than maximum number of trials in random search."},{"location":"errors/#error-13-when-random-search-has-a-stop-condition-ensembles-cant-be-applied","text":"","title":"ERROR 13: when random search has a stop condition, ensembles can't be applied."},{"location":"errors/#error-14-grid-search-feature-requires-the-number_of_attack_traces-2-x-key_rank_attack_traces","text":"","title":"ERROR 14: grid search feature requires the 'number_of_attack_traces' &gt;= 2 x key_rank_attack_traces."},{"location":"errors/#error-15-random-search-feature-requires-the-number_of_attack_traces-2-x-key_rank_attack_traces","text":"","title":"ERROR 15: random search feature requires the 'number_of_attack_traces' &gt;= 2 x key_rank_attack_traces."},{"location":"errors/#error-16-to-be-able-to-reproduce-analysis-results-user-must-set-random-states-for-ge-and-sr-with-set_ge_sr_random_states-method","text":"","title":"ERROR 16: to be able to reproduce analysis results, user must set random states for GE and SR with set_ge_sr_random_states() method"},{"location":"errors/#error-17-to-be-able-to-reproduce-analysis-results-user-must-set-random-states-for-ensembles-for-ge-and-sr-with-set_ge_sr_random_states_ensembles-method","text":"","title":"ERROR 17: to be able to reproduce analysis results, user must set random states for ensembles for GE and SR with set_ge_sr_random_states_ensembles() method"},{"location":"errors/#error-18-to-be-able-to-reproduce-analysis-results-user-must-set-random-states-for-hyper-parameters-search-for-ge-and-sr-with-set_ge_sr_random_states_search-method","text":"","title":"ERROR 18: to be able to reproduce analysis results, user must set random states for hyper-parameters search for GE and SR with set_ge_sr_random_states_search() method"},{"location":"errors/#error-19-to-be-able-to-reproduce-analysis-results-user-must-set-random-states-for-early-stopping-for-ge-and-sr-with-set_ge_sr_random_states_early_stopping-method","text":"","title":"ERROR 19: to be able to reproduce analysis results, user must set random states for early stopping for GE and SR with set_ge_sr_random_states_early_stopping() method"},{"location":"feature_request/","text":"Feature Request Would you like a new feature to be implemented in the AISY Framework. Please send a message to G.Perin@tudelft.nl and we will consider your request.","title":"Feature Request"},{"location":"feature_request/#feature-request","text":"Would you like a new feature to be implemented in the AISY Framework. Please send a message to G.Perin@tudelft.nl and we will consider your request.","title":"Feature Request"},{"location":"generating_scripts/","text":"Automatically Generating Scripts One of the main features in AISY Framework is the one-click script generation. By clicking on Databases on the side-bar, the list of analysis separated per database file name is displayed in the screen. The user then clicks on the chart icon in the Results column. The page then displays something similar to the image below: In the top right part of the screen, there is a button called Generate Script . If the user clicks on this button, the script used to produce results displayed in the current page is automatically generated and placed in scripts/ folder under the name script_[analysis_id]_[database_name].py . The advantage of this feature is the possibility to reproduce results from older analysis when script is updated or even modified by accident. Another advantage is that is databases are shared between users, the Generate Script button allows different users to reproduce same analysis.","title":"Automatically Generating Scripts"},{"location":"generating_scripts/#automatically-generating-scripts","text":"One of the main features in AISY Framework is the one-click script generation. By clicking on Databases on the side-bar, the list of analysis separated per database file name is displayed in the screen. The user then clicks on the chart icon in the Results column. The page then displays something similar to the image below: In the top right part of the screen, there is a button called Generate Script . If the user clicks on this button, the script used to produce results displayed in the current page is automatically generated and placed in scripts/ folder under the name script_[analysis_id]_[database_name].py . The advantage of this feature is the possibility to reproduce results from older analysis when script is updated or even modified by accident. Another advantage is that is databases are shared between users, the Generate Script button allows different users to reproduce same analysis.","title":"Automatically Generating Scripts"},{"location":"hyperparamatersearch/","text":"Hyperparameter Search Hyperparameter search a fundamental procedure in the application of deep learning to profiled side-channel analysis. AISY Framework provides easy and efficient functionalities for hyperparameters search for MLP and CNN models. Random Search Hyperparameter random search is the most common choice for finding efficient deep neural network architectures. Due to the very large search space, it is very likely that we have no enough computation power (and time) to cover all the search space. AISY Framework provides easy solution to run random search for profiled side-channel attacks. import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) # for each hyper-parameter, specify the min, max and step or the possible options random_search = { \"neural_network\": \"cnn\", \"hyper_parameters_search\": { 'conv_layers': {\"min\": 1, \"max\": 2, \"step\": 1}, 'kernel_1': {\"min\": 2, \"max\": 8, \"step\": 1}, 'kernel_2': {\"min\": 2, \"max\": 8, \"step\": 1}, 'stride_1': {\"min\": 5, \"max\": 10, \"step\": 5}, 'stride_2': {\"min\": 5, \"max\": 10, \"step\": 5}, 'filters_1': {\"min\": 8, \"max\": 32, \"step\": 4}, 'filters_2': {\"min\": 8, \"max\": 32, \"step\": 4}, 'pooling_type_1': [\"Average\", \"Max\"], 'pooling_type_2': [\"Average\", \"Max\"], 'pooling_size_1': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_size_2': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_stride_1': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_stride_2': {\"min\": 1, \"max\": 1, \"step\": 1}, 'neurons': {\"min\": 100, \"max\": 1000, \"step\": 100}, 'layers': {\"min\": 2, \"max\": 3, \"step\": 1}, 'learning_rate': [0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001], 'activation': [\"relu\", \"selu\", \"elu\", \"tanh\"], 'epochs': {\"min\": 5, \"max\": 5, \"step\": 1}, 'batch_size': {\"min\": 100, \"max\": 1000, \"step\": 100}, 'optimizer': [\"Adam\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"max_trials\": 10, \"train_after_search\": True } aisy.run( random_search=random_search, key_rank_attack_traces=500 ) As specified in the above examples, we need to pass a dictionary with the following parameters: neural_network: type of neural network. It can be \"mlp\" or cnn . hyper_parameters_search: dictionary containing the hyper-parameters to be search. Hyperparameters for MLP can be: neurons : dictionary containing the ranges for the number of neurons to be searched in dense layers. Example: 'neurons': {\"min\": 10, \"max\": 1000, \"step\": 10} , layers : dictionary containing the ranges for the number of dense layers. Example: 'layers': {\"min\": 1, \"max\": 10, \"step\": 1} , learning_rate : list containing the options for learning rate. Example: 'learning_rate': [0.001, 0.0001] , activation : list containing the options for activation functions. Example: 'activation': [\"relu\", \"selu\"] . Hyperparameters for CNN are the same as for MLP, expect for convolution and pooling layers: conv_layers: dictionary containing the ranges for the number of convolution layers. Example: 'conv_layers': {\"min\": 1, \"max\": 2, \"step\": 1} , kernel_1: dictionary containing the ranges for the kernel size in the conv_layer_1 . Example: 'kernel_1': {\"min\": 2, \"max\": 8, \"step\": 1} , kernel_2: dictionary containing the ranges for the kernel size in the conv_layer_2 . Example: 'kernel_2': {\"min\": 2, \"max\": 8, \"step\": 1} , stride_1: dictionary containing the ranges for the stride size in the conv_layer_1 . Example: 'stride_1': {\"min\": 1, \"max\": 2, \"step\": 1} , stride_2: dictionary containing the ranges for the stride size in the conv_layer_2 . Example: 'stride_2': {\"min\": 1, \"max\": 2, \"step\": 1} , filters_1: dictionary containing the ranges for the number of filters in the conv_layer_1 . Example: 'filters_1': {\"min\": 8, \"max\": 16, \"step\": 8} , filters_2: dictionary containing the ranges for the number of filters in the conv_layer_2 . Example: 'filters_2': {\"min\": 8, \"max\": 16, \"step\": 8} , pooling_type_1: list containing the options for pooling type in the pooling_layer_1 . Example: 'pooling_type_1': [\"Average\", \"Max\"] , pooling_type_2: list containing the options for pooling type in the pooling_layer_2 . Example: 'pooling_type_2': [\"Average\", \"Max\"] , pooling_size_1: dictionary containing the ranges for the pooling size in the pooling_layer_1 . Example: 'pooling_size_1': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_size_2: dictionary containing the ranges for the pooling size in the pooling_layer_2 . Example: 'pooling_size_2': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_stride_1: dictionary containing the ranges for the pooling stride in the pooling_layer_1 . Example: 'pooling_stride_1': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_stride_2: dictionary containing the ranges for the pooling stride in the pooling_layer_2 . Example: 'pooling_stride_2': {\"min\": 1, \"max\": 2, \"step\": 1} , Note that activation is set for convolution and dense layers. The structure of randomly generated CNNs follows the sequence with: conv_layer_1 -> pool_layer_1 -> batch_normalization_layer_1 ->conv_layer_2 -> pool_layer_2 -> batch_normalization_layer_2 -> ... where a BatchNormalization layer is defined after the pooling layer. Besides those specific hyperparameters for MLPs and CNNs, the user can also search for number of epochs and batch size: epochs: dictionary containing the ranges for the epochs. Example: 'epochs': {\"min\": 50, \"max\": 150, \"step\": 50} . batch_size: dictionary containing the ranges for the batch size. Example: 'batch_size': {\"min\": 100, \"max\": 4000, \"step\": 100} . optimizer: list of options for optimizers (must be Strings). Grid Search To perform a hyperparameter grid search, the main script needs to defined the ranges to be searched. The hyperparameter grid search in the AISY Framework supports two types of neural networks: MLP and CNN. Below we provide examples. MLP example: from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_dataset(\"ascad-variable.h5\") aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_number_of_profiling_traces(100000) aisy.set_number_of_attack_traces(2000) aisy.set_batch_size(400) aisy.set_epochs(50) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"mlp\", \"hyper_parameters_search\": { 'neurons': [100, 200], 'layers': [3, 4], 'learning_rate': [0.001, 0.0001], 'activation': [\"relu\", \"selu\"], 'optimizer': [\"Adam\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( grid_search=grid_search ) CNN example: import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"cnn\", \"hyper_parameters_search\": { 'conv_layers': [1, 2], 'kernel_1': [4, 8], 'kernel_2': [2, 4], 'stride_1': [1], 'stride_2': [1], 'filters_1': [8, 16], 'filters_2': [8, 16], 'pooling_type_1': [\"Average\", \"Max\"], 'pooling_type_2': [\"Average\", \"Max\"], 'pooling_size_1': [1, 2], 'pooling_size_2': [1, 2], 'pooling_stride_1': [1, 2], 'pooling_stride_2': [1, 2], 'neurons': [100, 200], 'layers': [3, 4], 'learning_rate': [0.001], 'activation': [\"selu\", \"elu\"], 'optimizer': [\"Adam\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( grid_search=grid_search, key_rank_attack_traces=500 ) As specified in the above examples, we need to pass a dictionary with the following parameters: neural_network: type of neural network. It can be \"mlp\" or cnn . hyper_parameters_search: dictionary containing the hyper-parameters to be search. Hyperparameters for MLP can be: neurons : list containing the options for the number of neurons to be searched in dense layers. Example: 'neurons': [100, 200] , layers : list containing the options for the number of dense layers. Example: 'layers': [3, 4] , learning_rate : list containing the options for learning rate. Example: 'learning_rate': [0.001, 0.0001] , activation : list containing the options for activation functions. Example: 'activation': [\"relu\", \"selu\"] . Hyperparameters for CNN are the same as for MLP, expect for convolution and pooling layers: conv_layers: list containing the options for the number of convolution layers. Example: 'conv_layers': [1, 2] , kernel_1: list containing the options for the kernel size in the conv_layer_1 . Example: 'kernel_1': [4, 8] , kernel_2: list containing the options for the kernel size in the conv_layer_2 . Example: 'kernel_2': [2, 4] , stride_1: list containing the options for the stride size in the conv_layer_1 . Example: 'stride_1': [1, 2] , stride_2: list containing the options for the stride size in the conv_layer_2 . Example: 'stride_2': [1, 2] , filters_1: list containing the options for the number of filters in the conv_layer_1 . Example: 'filters_1': [8, 16] , filters_2: list containing the options for the number of filters in the conv_layer_2 . Example: 'filters_2': [8, 16] , pooling_type_1: list containing the options for pooling type in the pooling_layer_1 . Example: 'pooling_type_1': [\"Average\", \"Max\"] , pooling_type_2: list containing the options for pooling type in the pooling_layer_2 . Example: 'pooling_type_2': [\"Average\", \"Max\"] , pooling_size_1: list containing the options for the pooling size in the pooling_layer_1 . Example: 'pooling_size_1': [1, 2] , pooling_size_2: list containing the options for the pooling size in the pooling_layer_2 . Example: 'pooling_size_2': [1, 2] , pooling_stride_1: list containing the options for the pooling stride in the pooling_layer_1 . Example: 'pooling_stride_1': [1, 2] , pooling_stride_2: list containing the options for the pooling stride in the pooling_layer_2 . Example: 'pooling_stride_2': [1, 2] , Note that activation is set for convolution and dense layers. The structure of randomly generated CNNs follows the sequence with: conv_layer_1 -> pool_layer_1 -> batch_normalization_layer_1 ->conv_layer_2 -> pool_layer_2 -> batch_normalization_layer_2 -> ... where a BatchNormalization layer is defined after the pooling layer. Besides those specific hyperparameters for MLPs and CNNs, the user can also search for number of epochs and batch size: epochs: list containing the options for the epochs. Example: 'epochs': [50, 100, 150] . batch_size: list containing the option for the batch size. Example: 'batch_size': [100, 200] . optimizer: list of options for optimizers (must be Strings).","title":"Hyperparameter Search"},{"location":"hyperparamatersearch/#hyperparameter-search","text":"Hyperparameter search a fundamental procedure in the application of deep learning to profiled side-channel analysis. AISY Framework provides easy and efficient functionalities for hyperparameters search for MLP and CNN models.","title":"Hyperparameter Search"},{"location":"hyperparamatersearch/#random-search","text":"Hyperparameter random search is the most common choice for finding efficient deep neural network architectures. Due to the very large search space, it is very likely that we have no enough computation power (and time) to cover all the search space. AISY Framework provides easy solution to run random search for profiled side-channel attacks. import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(10) # for each hyper-parameter, specify the min, max and step or the possible options random_search = { \"neural_network\": \"cnn\", \"hyper_parameters_search\": { 'conv_layers': {\"min\": 1, \"max\": 2, \"step\": 1}, 'kernel_1': {\"min\": 2, \"max\": 8, \"step\": 1}, 'kernel_2': {\"min\": 2, \"max\": 8, \"step\": 1}, 'stride_1': {\"min\": 5, \"max\": 10, \"step\": 5}, 'stride_2': {\"min\": 5, \"max\": 10, \"step\": 5}, 'filters_1': {\"min\": 8, \"max\": 32, \"step\": 4}, 'filters_2': {\"min\": 8, \"max\": 32, \"step\": 4}, 'pooling_type_1': [\"Average\", \"Max\"], 'pooling_type_2': [\"Average\", \"Max\"], 'pooling_size_1': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_size_2': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_stride_1': {\"min\": 1, \"max\": 1, \"step\": 1}, 'pooling_stride_2': {\"min\": 1, \"max\": 1, \"step\": 1}, 'neurons': {\"min\": 100, \"max\": 1000, \"step\": 100}, 'layers': {\"min\": 2, \"max\": 3, \"step\": 1}, 'learning_rate': [0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001], 'activation': [\"relu\", \"selu\", \"elu\", \"tanh\"], 'epochs': {\"min\": 5, \"max\": 5, \"step\": 1}, 'batch_size': {\"min\": 100, \"max\": 1000, \"step\": 100}, 'optimizer': [\"Adam\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"max_trials\": 10, \"train_after_search\": True } aisy.run( random_search=random_search, key_rank_attack_traces=500 ) As specified in the above examples, we need to pass a dictionary with the following parameters: neural_network: type of neural network. It can be \"mlp\" or cnn . hyper_parameters_search: dictionary containing the hyper-parameters to be search. Hyperparameters for MLP can be: neurons : dictionary containing the ranges for the number of neurons to be searched in dense layers. Example: 'neurons': {\"min\": 10, \"max\": 1000, \"step\": 10} , layers : dictionary containing the ranges for the number of dense layers. Example: 'layers': {\"min\": 1, \"max\": 10, \"step\": 1} , learning_rate : list containing the options for learning rate. Example: 'learning_rate': [0.001, 0.0001] , activation : list containing the options for activation functions. Example: 'activation': [\"relu\", \"selu\"] . Hyperparameters for CNN are the same as for MLP, expect for convolution and pooling layers: conv_layers: dictionary containing the ranges for the number of convolution layers. Example: 'conv_layers': {\"min\": 1, \"max\": 2, \"step\": 1} , kernel_1: dictionary containing the ranges for the kernel size in the conv_layer_1 . Example: 'kernel_1': {\"min\": 2, \"max\": 8, \"step\": 1} , kernel_2: dictionary containing the ranges for the kernel size in the conv_layer_2 . Example: 'kernel_2': {\"min\": 2, \"max\": 8, \"step\": 1} , stride_1: dictionary containing the ranges for the stride size in the conv_layer_1 . Example: 'stride_1': {\"min\": 1, \"max\": 2, \"step\": 1} , stride_2: dictionary containing the ranges for the stride size in the conv_layer_2 . Example: 'stride_2': {\"min\": 1, \"max\": 2, \"step\": 1} , filters_1: dictionary containing the ranges for the number of filters in the conv_layer_1 . Example: 'filters_1': {\"min\": 8, \"max\": 16, \"step\": 8} , filters_2: dictionary containing the ranges for the number of filters in the conv_layer_2 . Example: 'filters_2': {\"min\": 8, \"max\": 16, \"step\": 8} , pooling_type_1: list containing the options for pooling type in the pooling_layer_1 . Example: 'pooling_type_1': [\"Average\", \"Max\"] , pooling_type_2: list containing the options for pooling type in the pooling_layer_2 . Example: 'pooling_type_2': [\"Average\", \"Max\"] , pooling_size_1: dictionary containing the ranges for the pooling size in the pooling_layer_1 . Example: 'pooling_size_1': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_size_2: dictionary containing the ranges for the pooling size in the pooling_layer_2 . Example: 'pooling_size_2': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_stride_1: dictionary containing the ranges for the pooling stride in the pooling_layer_1 . Example: 'pooling_stride_1': {\"min\": 1, \"max\": 2, \"step\": 1} , pooling_stride_2: dictionary containing the ranges for the pooling stride in the pooling_layer_2 . Example: 'pooling_stride_2': {\"min\": 1, \"max\": 2, \"step\": 1} , Note that activation is set for convolution and dense layers. The structure of randomly generated CNNs follows the sequence with: conv_layer_1 -> pool_layer_1 -> batch_normalization_layer_1 ->conv_layer_2 -> pool_layer_2 -> batch_normalization_layer_2 -> ... where a BatchNormalization layer is defined after the pooling layer. Besides those specific hyperparameters for MLPs and CNNs, the user can also search for number of epochs and batch size: epochs: dictionary containing the ranges for the epochs. Example: 'epochs': {\"min\": 50, \"max\": 150, \"step\": 50} . batch_size: dictionary containing the ranges for the batch size. Example: 'batch_size': {\"min\": 100, \"max\": 4000, \"step\": 100} . optimizer: list of options for optimizers (must be Strings).","title":"Random Search"},{"location":"hyperparamatersearch/#grid-search","text":"To perform a hyperparameter grid search, the main script needs to defined the ranges to be searched. The hyperparameter grid search in the AISY Framework supports two types of neural networks: MLP and CNN. Below we provide examples. MLP example: from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_dataset(\"ascad-variable.h5\") aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_number_of_profiling_traces(100000) aisy.set_number_of_attack_traces(2000) aisy.set_batch_size(400) aisy.set_epochs(50) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"mlp\", \"hyper_parameters_search\": { 'neurons': [100, 200], 'layers': [3, 4], 'learning_rate': [0.001, 0.0001], 'activation': [\"relu\", \"selu\"], 'optimizer': [\"Adam\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( grid_search=grid_search ) CNN example: import aisy_sca from app import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) # for each hyper-parameter, specify the options in the grid search grid_search = { \"neural_network\": \"cnn\", \"hyper_parameters_search\": { 'conv_layers': [1, 2], 'kernel_1': [4, 8], 'kernel_2': [2, 4], 'stride_1': [1], 'stride_2': [1], 'filters_1': [8, 16], 'filters_2': [8, 16], 'pooling_type_1': [\"Average\", \"Max\"], 'pooling_type_2': [\"Average\", \"Max\"], 'pooling_size_1': [1, 2], 'pooling_size_2': [1, 2], 'pooling_stride_1': [1, 2], 'pooling_stride_2': [1, 2], 'neurons': [100, 200], 'layers': [3, 4], 'learning_rate': [0.001], 'activation': [\"selu\", \"elu\"], 'optimizer': [\"Adam\", \"SGD\"] }, \"metric\": \"guessing_entropy\", \"stop_condition\": False, \"stop_value\": 1.0, \"train_after_search\": True } aisy.run( grid_search=grid_search, key_rank_attack_traces=500 ) As specified in the above examples, we need to pass a dictionary with the following parameters: neural_network: type of neural network. It can be \"mlp\" or cnn . hyper_parameters_search: dictionary containing the hyper-parameters to be search. Hyperparameters for MLP can be: neurons : list containing the options for the number of neurons to be searched in dense layers. Example: 'neurons': [100, 200] , layers : list containing the options for the number of dense layers. Example: 'layers': [3, 4] , learning_rate : list containing the options for learning rate. Example: 'learning_rate': [0.001, 0.0001] , activation : list containing the options for activation functions. Example: 'activation': [\"relu\", \"selu\"] . Hyperparameters for CNN are the same as for MLP, expect for convolution and pooling layers: conv_layers: list containing the options for the number of convolution layers. Example: 'conv_layers': [1, 2] , kernel_1: list containing the options for the kernel size in the conv_layer_1 . Example: 'kernel_1': [4, 8] , kernel_2: list containing the options for the kernel size in the conv_layer_2 . Example: 'kernel_2': [2, 4] , stride_1: list containing the options for the stride size in the conv_layer_1 . Example: 'stride_1': [1, 2] , stride_2: list containing the options for the stride size in the conv_layer_2 . Example: 'stride_2': [1, 2] , filters_1: list containing the options for the number of filters in the conv_layer_1 . Example: 'filters_1': [8, 16] , filters_2: list containing the options for the number of filters in the conv_layer_2 . Example: 'filters_2': [8, 16] , pooling_type_1: list containing the options for pooling type in the pooling_layer_1 . Example: 'pooling_type_1': [\"Average\", \"Max\"] , pooling_type_2: list containing the options for pooling type in the pooling_layer_2 . Example: 'pooling_type_2': [\"Average\", \"Max\"] , pooling_size_1: list containing the options for the pooling size in the pooling_layer_1 . Example: 'pooling_size_1': [1, 2] , pooling_size_2: list containing the options for the pooling size in the pooling_layer_2 . Example: 'pooling_size_2': [1, 2] , pooling_stride_1: list containing the options for the pooling stride in the pooling_layer_1 . Example: 'pooling_stride_1': [1, 2] , pooling_stride_2: list containing the options for the pooling stride in the pooling_layer_2 . Example: 'pooling_stride_2': [1, 2] , Note that activation is set for convolution and dense layers. The structure of randomly generated CNNs follows the sequence with: conv_layer_1 -> pool_layer_1 -> batch_normalization_layer_1 ->conv_layer_2 -> pool_layer_2 -> batch_normalization_layer_2 -> ... where a BatchNormalization layer is defined after the pooling layer. Besides those specific hyperparameters for MLPs and CNNs, the user can also search for number of epochs and batch size: epochs: list containing the options for the epochs. Example: 'epochs': [50, 100, 150] . batch_size: list containing the option for the batch size. Example: 'batch_size': [100, 200] . optimizer: list of options for optimizers (must be Strings).","title":"Grid Search"},{"location":"leakage_models/","text":"Leakage Models AISY Framework v0.1 only has support for AES128 leakage models. Future releases will include leakage models support for additional ciphers. Definiting Leakage Models In the main script, the user needs to call the method set_aes_leakage_model() and pass AES leakage model attributes. The attributes are: leakage_model: Hamming weight (\"HW\"), Hamming distance (\"HD\"), Identity (\"ID\") or \"bit\". bit: index of the target bit in a byte (the byte attribute also has to be defined if bit model is set). byte: index of the target byte in a AES state. round: index of the target round in AES. round_first: index of the first target round in AES when Hamming distance is set as leakage model. round_second: index of the second target round in AES when Hamming distance is set as leakage model. Intermediates obtained from round_first and round_second are XORed. target_state: string specifying the target AES state. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. target_state_first: string specifying the first target AES state when Hamming distance is set as leakage model. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. target_state_second: string specifying the second target AES state when Hamming distance is set as leakage model. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. direction: string defining the AES direction. Supported values are \"Encryption\" and \"Decryption\". attack_direction: string defining the attack direction to create the leakage models. Supported values are \"input\" and \"output\", both lowercase. Supported AES States for Leakage Models Figure below illustrates the AES states that the user can target in leakage model definitions. Default Values The framework set the following as default ones: leakage_model = { \"leakage_model\": \"HW\", \"bit\": 0, \"byte\": 0, \"round\": 1, \"round_first\": 1, # for Hamming Distance \"round_second\": 1, # for Hamming Distance \"cipher\": \"AES128\", \"target_state\": \"Sbox\", \"target_state_first\": \"Sbox\", # for Hamming Distance \"target_state_second\": \"Sbox\", # for Hamming Distance \"direction\": \"Encryption\", \"attack_direction\": \"input\" } If the default values are the same as the user needs to select, the set_aes_leakage_model() method does not need to be set in the main script. Examples Hamming weight model, S-Box Output Round 1, AES Encryption from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=0, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\") Hamming weight model, S-Box Input Round 10, AES Encryption from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=0, target_state=\"Sbox\", round=10, attack_direction=\"output\", direction=\"Encryption\", cipher=\"AES128\") Identity model, S-Box Output Round 1, AES Encryption from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"ID\", byte=0, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\") Bit 3, Byte 5, S-Box Output Round 1, AES Encryption from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"bit\", bit=3, byte=5, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\") Hamming distance between S-Box Input in round 10 and output (ciphertext), AES Encryption, byte 0 from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HD\", byte=0, direction=\"Encryption\", cipher=\"AES128\", target_state_first=\"Output\", round_first=10, target_state_second=\"Sbox\", round_second=10, attack_direction=\"output\")","title":"Leakage Models"},{"location":"leakage_models/#leakage-models","text":"AISY Framework v0.1 only has support for AES128 leakage models. Future releases will include leakage models support for additional ciphers.","title":"Leakage Models"},{"location":"leakage_models/#definiting-leakage-models","text":"In the main script, the user needs to call the method set_aes_leakage_model() and pass AES leakage model attributes. The attributes are: leakage_model: Hamming weight (\"HW\"), Hamming distance (\"HD\"), Identity (\"ID\") or \"bit\". bit: index of the target bit in a byte (the byte attribute also has to be defined if bit model is set). byte: index of the target byte in a AES state. round: index of the target round in AES. round_first: index of the first target round in AES when Hamming distance is set as leakage model. round_second: index of the second target round in AES when Hamming distance is set as leakage model. Intermediates obtained from round_first and round_second are XORed. target_state: string specifying the target AES state. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. target_state_first: string specifying the first target AES state when Hamming distance is set as leakage model. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. target_state_second: string specifying the second target AES state when Hamming distance is set as leakage model. Supported values are: 'Input', 'Output', 'Sbox', InvSbox', 'AddRoundKey', 'MixColumns', 'InvMixColumns', 'ShiftRows' and 'InvShiftRows'. direction: string defining the AES direction. Supported values are \"Encryption\" and \"Decryption\". attack_direction: string defining the attack direction to create the leakage models. Supported values are \"input\" and \"output\", both lowercase.","title":"Definiting Leakage Models"},{"location":"leakage_models/#supported-aes-states-for-leakage-models","text":"Figure below illustrates the AES states that the user can target in leakage model definitions.","title":"Supported AES States for Leakage Models"},{"location":"leakage_models/#default-values","text":"The framework set the following as default ones: leakage_model = { \"leakage_model\": \"HW\", \"bit\": 0, \"byte\": 0, \"round\": 1, \"round_first\": 1, # for Hamming Distance \"round_second\": 1, # for Hamming Distance \"cipher\": \"AES128\", \"target_state\": \"Sbox\", \"target_state_first\": \"Sbox\", # for Hamming Distance \"target_state_second\": \"Sbox\", # for Hamming Distance \"direction\": \"Encryption\", \"attack_direction\": \"input\" } If the default values are the same as the user needs to select, the set_aes_leakage_model() method does not need to be set in the main script.","title":"Default Values"},{"location":"leakage_models/#examples","text":"","title":"Examples"},{"location":"leakage_models/#hamming-weight-model-s-box-output-round-1-aes-encryption","text":"from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=0, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\")","title":"Hamming weight model, S-Box Output Round 1, AES Encryption"},{"location":"leakage_models/#hamming-weight-model-s-box-input-round-10-aes-encryption","text":"from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=0, target_state=\"Sbox\", round=10, attack_direction=\"output\", direction=\"Encryption\", cipher=\"AES128\")","title":"Hamming weight model, S-Box Input Round 10, AES Encryption"},{"location":"leakage_models/#identity-model-s-box-output-round-1-aes-encryption","text":"from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"ID\", byte=0, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\")","title":"Identity model, S-Box Output Round 1, AES Encryption"},{"location":"leakage_models/#bit-3-byte-5-s-box-output-round-1-aes-encryption","text":"from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"bit\", bit=3, byte=5, target_state=\"Sbox\", direction=\"Encryption\", cipher=\"AES128\")","title":"Bit 3, Byte 5,  S-Box Output Round 1, AES Encryption"},{"location":"leakage_models/#hamming-distance-between-s-box-input-in-round-10-and-output-ciphertext-aes-encryption-byte-0","text":"from aisy.sca_aisy_aes import AisyAes aisy = AisyAes() aisy.set_aes_leakage_model(leakage_model=\"HD\", byte=0, direction=\"Encryption\", cipher=\"AES128\", target_state_first=\"Output\", round_first=10, target_state_second=\"Sbox\", round_second=10, attack_direction=\"output\")","title":"Hamming distance between S-Box Input in round 10 and output (ciphertext), AES Encryption, byte 0"},{"location":"neuralnetworks/","text":"Creating Neural Network Models The AISY Framework is a (Keras/TensorFlow) deep learning-based framework for profiled side-channel analysis. One of the main advantages of using our framework is the easy definition and call of neural network models. Keras class support The framework support Keras models written using either Sequential or Model classes. Custom Neural Networks File For better code organization, we recommend writing all your (keras) neural network models in the custom/custom_models/neural_network.py file. The standard code is provided below: from tensorflow.keras.optimizers import * from tensorflow.keras.layers import * from tensorflow.keras.models import * def cnn(classes, number_of_samples): model = Sequential() model.add(Conv1D(filters=8, kernel_size=20, strides=1, activation='relu', padding='valid', input_shape=(number_of_samples, 1))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros')) model.add(Dropout(0.5)) model.add(Dense(128, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros')) model.add(Dense(classes, activation='softmax')) model.summary() optimizer = Adam(lr=0.001) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model def mlp(classes, number_of_samples): model = Sequential() model.add(Dense(200, activation='selu', input_shape=(number_of_samples,))) model.add(Dense(200, activation='selu')) model.add(Dense(200, activation='selu')) model.add(Dense(200, activation='selu')) model.add(Dense(classes, activation='softmax')) model.summary() optimizer = Adam(lr=0.001) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model Note the neural network functions must be defined with two basic method parameters: classes: number of classification classes. This is important in order to define the length of the output layer. number_of_samples: number of samples in each trace. This is important in order to define the length of the input layer. Following, the defined neural network models can be easily called from the main script, as in the example below: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() As shown in the above code, the neural network model is defined by setting the method: aisy.set_neural_network(mlp) Note, however, that when the neural network model is set as above, the parameters classes and number_of_samples don't need to be passed. They will be automatically set according to the defined dataset and leakage model settings. Passing Custom Parameters to the Neural Network Definition The user is also able to define and set a neural network model with special parameters, as in the example below, where we can create models with flexible hyper-parameters: import aisy_sca from app import * from custom.custom_models.neural_networks import * def mlp(classes, number_of_samples, neuron, layer, activation, learning_rate): model = Sequential(name=\"my_mlp\") for l_i in range(layer): if l_i == 0: model.add(Dense(neuron, activation=activation, input_shape=(number_of_samples,))) else: model.add(Dense(neuron, activation=activation)) model.add(Dense(classes, activation=None)) model.add(Activation(activation=\"softmax\")) model.summary() optimizer = Adam(lr=learning_rate) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(40) my_mlp = mlp(9, 1400, 200, 6, \"relu\", 0.001) aisy.set_neural_network(my_mlp) aisy.run() Note that in this case, it is necessary to pass all function parameters, including classes and number_of_samples .","title":"Creating Neural Networks"},{"location":"neuralnetworks/#creating-neural-network-models","text":"The AISY Framework is a (Keras/TensorFlow) deep learning-based framework for profiled side-channel analysis. One of the main advantages of using our framework is the easy definition and call of neural network models.","title":"Creating Neural Network Models"},{"location":"neuralnetworks/#keras-class-support","text":"The framework support Keras models written using either Sequential or Model classes.","title":"Keras class support"},{"location":"neuralnetworks/#custom-neural-networks-file","text":"For better code organization, we recommend writing all your (keras) neural network models in the custom/custom_models/neural_network.py file. The standard code is provided below: from tensorflow.keras.optimizers import * from tensorflow.keras.layers import * from tensorflow.keras.models import * def cnn(classes, number_of_samples): model = Sequential() model.add(Conv1D(filters=8, kernel_size=20, strides=1, activation='relu', padding='valid', input_shape=(number_of_samples, 1))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros')) model.add(Dropout(0.5)) model.add(Dense(128, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros')) model.add(Dense(classes, activation='softmax')) model.summary() optimizer = Adam(lr=0.001) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model def mlp(classes, number_of_samples): model = Sequential() model.add(Dense(200, activation='selu', input_shape=(number_of_samples,))) model.add(Dense(200, activation='selu')) model.add(Dense(200, activation='selu')) model.add(Dense(200, activation='selu')) model.add(Dense(classes, activation='softmax')) model.summary() optimizer = Adam(lr=0.001) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model Note the neural network functions must be defined with two basic method parameters: classes: number of classification classes. This is important in order to define the length of the output layer. number_of_samples: number of samples in each trace. This is important in order to define the length of the input layer. Following, the defined neural network models can be easily called from the main script, as in the example below: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run() As shown in the above code, the neural network model is defined by setting the method: aisy.set_neural_network(mlp) Note, however, that when the neural network model is set as above, the parameters classes and number_of_samples don't need to be passed. They will be automatically set according to the defined dataset and leakage model settings.","title":"Custom Neural Networks File"},{"location":"neuralnetworks/#passing-custom-parameters-to-the-neural-network-definition","text":"The user is also able to define and set a neural network model with special parameters, as in the example below, where we can create models with flexible hyper-parameters: import aisy_sca from app import * from custom.custom_models.neural_networks import * def mlp(classes, number_of_samples, neuron, layer, activation, learning_rate): model = Sequential(name=\"my_mlp\") for l_i in range(layer): if l_i == 0: model.add(Dense(neuron, activation=activation, input_shape=(number_of_samples,))) else: model.add(Dense(neuron, activation=activation)) model.add(Dense(classes, activation=None)) model.add(Activation(activation=\"softmax\")) model.summary() optimizer = Adam(lr=learning_rate) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(40) my_mlp = mlp(9, 1400, 200, 6, \"relu\", 0.001) aisy.set_neural_network(my_mlp) aisy.run() Note that in this case, it is necessary to pass all function parameters, including classes and number_of_samples .","title":"Passing Custom Parameters to the Neural Network Definition"},{"location":"profiling_analyzer/","text":"Profiling Analyzer ProfilingAnalyzer class provides a full implementation of Efficient Attacker Framework . This feature allows a more detailed evaluation of a neural network behavior for profiling side-channel analysis. In profiling SCA, the evaluator or the implementer want to evaluate the security of a target device against a strong adversary. When using a deep neural network as profiling model, one wants to understand how correct is the profiling model. There are multiple objectives in this analysis: Understanding overfitting : to understand if a profiling model is actually overfitting due to small amount of profiling traces used for training, it is necessary to evaluate the model for different amounts of profiling traces. Understanding generalization of the model : one way to verify the generalizing level of a profiling model is by predicting multiple attack sets with the model. For all attack sets, guessing entropy is evaluated. To verify how what is the model generalization, it is important to train this model for multiple amounts of profiling traces. Impact of hyperparameters/learnability : neural networks can be configured with infinite amount of hyperparameters combinations. In a profiling SCA, it is important to verify how the search for different models impacts the attack performance (overfitting and generalization). To address these objectives, ProfilingAnalyzer class provides the following results: Guessing entropy vs profiling traces Success rate vs profiling traces Attack traces (for GE=1) vs profiling traces Code below provides an example of how to call profiling analyzer from the main script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run( profiling_analyzer= { \"steps\": [1000, 5000, 10000, 25000, 50000] } ) The steps argument indicates for which amounts of profiling traces the model will trained and evaluated.","title":"Profiling Analyzer"},{"location":"profiling_analyzer/#profiling-analyzer","text":"ProfilingAnalyzer class provides a full implementation of Efficient Attacker Framework . This feature allows a more detailed evaluation of a neural network behavior for profiling side-channel analysis. In profiling SCA, the evaluator or the implementer want to evaluate the security of a target device against a strong adversary. When using a deep neural network as profiling model, one wants to understand how correct is the profiling model. There are multiple objectives in this analysis: Understanding overfitting : to understand if a profiling model is actually overfitting due to small amount of profiling traces used for training, it is necessary to evaluate the model for different amounts of profiling traces. Understanding generalization of the model : one way to verify the generalizing level of a profiling model is by predicting multiple attack sets with the model. For all attack sets, guessing entropy is evaluated. To verify how what is the model generalization, it is important to train this model for multiple amounts of profiling traces. Impact of hyperparameters/learnability : neural networks can be configured with infinite amount of hyperparameters combinations. In a profiling SCA, it is important to verify how the search for different models impacts the attack performance (overfitting and generalization). To address these objectives, ProfilingAnalyzer class provides the following results: Guessing entropy vs profiling traces Success rate vs profiling traces Attack traces (for GE=1) vs profiling traces Code below provides an example of how to call profiling analyzer from the main script: import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run( profiling_analyzer= { \"steps\": [1000, 5000, 10000, 25000, 50000] } ) The steps argument indicates for which amounts of profiling traces the model will trained and evaluated.","title":"Profiling Analyzer"},{"location":"simple_example/","text":"Simple Example The following example shows how simple it is to break ASCAD Random Keys dataset with AISY Framework. The example considers the dataset ascad_variable.h5 provided in the original ASCAD Github repository. This dataset is a trimmed version of side-channel traces containing 1400 features that represent the processing of third Sbox operation during first encryption round. Note that this operation is masked with first-order Boolean masking scheme (see Ref ). Code import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run()","title":"Simple Example"},{"location":"simple_example/#simple-example","text":"The following example shows how simple it is to break ASCAD Random Keys dataset with AISY Framework. The example considers the dataset ascad_variable.h5 provided in the original ASCAD Github repository. This dataset is a trimmed version of side-channel traces containing 1400 features that represent the processing of third Sbox operation during first encryption round. Note that this operation is masked with first-order Boolean masking scheme (see Ref ).","title":"Simple Example"},{"location":"simple_example/#code","text":"import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run()","title":"Code"},{"location":"visualization/","text":"Visualization AISY Framework provides an input gradient visualization feature. The feature allows the visual verification of main input features learned from input traces. Example Code import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run(visualization=[4000]) The amount 4000 represents the number of profiling traces that are used to compute input gradient. Results are given in two ways, as explained below. Sum of Input Gradients The sum of input gradients provides the sum of input gradients computed over all preset profiling traces and for all the processed epochs. In the web application, results should appear as shown below: Input Gradients per Epoch The web application also provides the input gradient computed over all preset profiling traces for each epoch in a heatmap plot, as shown below:","title":"Visualization"},{"location":"visualization/#visualization","text":"AISY Framework provides an input gradient visualization feature. The feature allows the visual verification of main input features learned from input traces.","title":"Visualization"},{"location":"visualization/#example-code","text":"import aisy_sca from app import * from custom.custom_models.neural_networks import * aisy = aisy_sca.Aisy() aisy.set_resources_root_folder(resources_root_folder) aisy.set_database_root_folder(databases_root_folder) aisy.set_datasets_root_folder(datasets_root_folder) aisy.set_database_name(\"database_ascad.sqlite\") aisy.set_dataset(datasets_dict[\"ascad-variable.h5\"]) aisy.set_aes_leakage_model(leakage_model=\"HW\", byte=2) aisy.set_batch_size(400) aisy.set_epochs(20) aisy.set_neural_network(mlp) aisy.run(visualization=[4000]) The amount 4000 represents the number of profiling traces that are used to compute input gradient. Results are given in two ways, as explained below.","title":"Example Code"},{"location":"visualization/#sum-of-input-gradients","text":"The sum of input gradients provides the sum of input gradients computed over all preset profiling traces and for all the processed epochs. In the web application, results should appear as shown below:","title":"Sum of Input Gradients"},{"location":"visualization/#input-gradients-per-epoch","text":"The web application also provides the input gradient computed over all preset profiling traces for each epoch in a heatmap plot, as shown below:","title":"Input Gradients per Epoch"}]}